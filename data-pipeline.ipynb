{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "1. Topic Extraction\n",
    "2. User Buckets\n",
    "3. Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Dataset description\n",
    "||Feature Name|Feature Type|Feature Description|\n",
    "|:---|:---|:---|:---|\n",
    "|Tweet Features|Text tokens<br>Hashtags<br>Tweet id<br>Present media<br>Present links<br>Present domains<br>Tweet type<br>Language<br>Timestamp|List[long]<br>List[string]<br>String<br>List[String]<br>List[string]<br>List[string]<br>String<br>String<br>Long|Ordered list of Bert ids corresponding to Bert tokenization of Tweet text<br>Tab separated list of hastags (identifiers) present in the tweet<br>Tweet identifier<br>Tab separated list of media types. Media type can be in (Photo, Video, Gif)<br>Tab separeted list of links (identifiers) included in the Tweet<br>Tab separated list of domains included in the Tweet (twitter.com, dogs.com)<br>Tweet type, can be either Retweet, Quote, Reply, or Toplevel<br>Identifier corresponding to the inferred language of the Tweet<br>Unix timestamp, in sec of the creation time of the Tweet|\n",
    "|Engaged With User Features|User id<br>Follower count<br>Following count<br>Is verified?<br>Account creation time|String<br>Long<br>Long<br>Bool<br>Long|User identifier<br>Number of followers of the user<br>Number of accounts<br>the user is following<br>Is the account verified?<br>Unix timestamp, in seconds, of the creation time of the account\n",
    "|Engaging User Features|User id<br>Follower count<br>Following count<br>Is verified?<br>Account creation time|String<br>Long<br>Long<br>Bool<br>Long|User identifier<br>Number of followers of the user<br>Number of accounts<br>the user is following<br>Is the account verified?<br>Unix timestamp, in seconds, of the creation time of the account\n",
    "|Engagement Features|Engagee follows engager?<br>Reply engagement timestamp<br>Retweet engagement timestamp<br>Retweet<br>with comment engagement timestamp<br>Like engagement timestamp|Bool<br>Long<br>Long<br>Long<br>Long|Does the account of the engaged tweet author follow the account that has made the engagement?<br>If there is at least one, unix timestamp, in s, of one of the replies<br>If there is one, unix timestamp, in s, of the retweet of the tweet by the engaging user<br>If there is at least one, unix timestamp, in s, of one of the retweet with comment of the tweet by the engaging user<br>If there is one, Unix timestamp, in s, of the like\n",
    "\n",
    "참고) https://recsys-twitter.com/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "COLS = [\"text_tokens\", \"hashtags\", \"tweet_id\", \"present_media\", \n",
    "                      \"present_links\", \"present_domains\", \"tweet_type\", \"language\", \"tweet_timestamp\", \"engaged_with_user_id\",\n",
    "                     \"engaged_with_user_follower_count\", \"engaged_with_user_following_count\", \n",
    "                     \"engaged_with_user_is_verified\", \"engaged_with_user_account_creation\", \"engaging_user_id\",\n",
    "                     \"engaging_user_follower_count\", \"engaging_user_following_count\", \"engaging_user_is_verified\",\n",
    "                     \"engaging_user_account_creation\", \"engagee_follows_engager\", \"reply_timestamp\",\n",
    "                     \"retweet_timestamp\", \"retweet_with_comment_timestamp\", \"like_timestamp\"]\n",
    "df = pd.read_csv('../dataset/twitter/train100K.csv', names=COLS, skipinitialspace=True, skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>present_media</th>\n",
       "      <th>present_links</th>\n",
       "      <th>present_domains</th>\n",
       "      <th>tweet_type</th>\n",
       "      <th>language</th>\n",
       "      <th>tweet_timestamp</th>\n",
       "      <th>engaged_with_user_id</th>\n",
       "      <th>...</th>\n",
       "      <th>engaging_user_id</th>\n",
       "      <th>engaging_user_follower_count</th>\n",
       "      <th>engaging_user_following_count</th>\n",
       "      <th>engaging_user_is_verified</th>\n",
       "      <th>engaging_user_account_creation</th>\n",
       "      <th>engagee_follows_engager</th>\n",
       "      <th>reply_timestamp</th>\n",
       "      <th>retweet_timestamp</th>\n",
       "      <th>retweet_with_comment_timestamp</th>\n",
       "      <th>like_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101\\t10117\\t140\\t119\\t142\\t119\\t152\\t119\\t1010...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>373C0F43762B7CEC1D75728BE8A33891</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A2CE3A1941BA410A1C31496C355EFCD7</td>\n",
       "      <td>E14AF8A8D257BB47587843FE7D08382B</td>\n",
       "      <td>TopLevel</td>\n",
       "      <td>D3164C7FBCF2565DDF915B1B3AEFB1DC</td>\n",
       "      <td>1582126349</td>\n",
       "      <td>2A8B6AD2B9D55F535C2441AB673133D2</td>\n",
       "      <td>...</td>\n",
       "      <td>00000865A1538142CDA5936B07FE4311</td>\n",
       "      <td>65</td>\n",
       "      <td>166</td>\n",
       "      <td>False</td>\n",
       "      <td>1452599043</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101\\t10105\\t10817\\t10124\\t59232\\t18121\\t15629\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>773A92D9E4824D06105C02BD044BB20A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Quote</td>\n",
       "      <td>D3164C7FBCF2565DDF915B1B3AEFB1DC</td>\n",
       "      <td>1581971193</td>\n",
       "      <td>950A95B81407F33C412E520BE55A1450</td>\n",
       "      <td>...</td>\n",
       "      <td>000009A057792FF118B9E3F2578B8407</td>\n",
       "      <td>1814</td>\n",
       "      <td>1314</td>\n",
       "      <td>False</td>\n",
       "      <td>1322868747</td>\n",
       "      <td>True</td>\n",
       "      <td>1.581979e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101\\t48561\\t10116\\t67737\\t18554\\t36371\\t10989\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>218A6C27871801759F7380D7C41694A6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5C683B5A29B308CADD0D7EFA7C9C32D3</td>\n",
       "      <td>6717B03E03DEE1D7ACAE37649ACA7BD6</td>\n",
       "      <td>TopLevel</td>\n",
       "      <td>9BF3403E0EB7EA8A256DA9019C0B0716</td>\n",
       "      <td>1582047119</td>\n",
       "      <td>ABB2F7F22C34057BC7B30D627B0C137A</td>\n",
       "      <td>...</td>\n",
       "      <td>00000DEF82BE9EB5CFD07FB7DB94317B</td>\n",
       "      <td>4</td>\n",
       "      <td>73</td>\n",
       "      <td>False</td>\n",
       "      <td>1573996260</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101\\t100055\\t69940\\t10414\\t159\\t11305\\t11166\\t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AB817EBA68064A0C8CBF4A6C059D92DC</td>\n",
       "      <td>Photo</td>\n",
       "      <td>E925556EE312213AD98C4D9F131D7A8D</td>\n",
       "      <td>D722330FEBEAAE68B4F4339CE8BD7C70</td>\n",
       "      <td>TopLevel</td>\n",
       "      <td>691890251F2B9FF922BE6D3699ABEFD2</td>\n",
       "      <td>1581554925</td>\n",
       "      <td>03F96C3B7CE2179B6347AA395880C963</td>\n",
       "      <td>...</td>\n",
       "      <td>0000109A57AFA64758EE4AAE2A01BFC7</td>\n",
       "      <td>15</td>\n",
       "      <td>124</td>\n",
       "      <td>False</td>\n",
       "      <td>1385502405</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101\\t62154\\t32221\\t71843\\t10143\\t10237\\t15507\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>349120C1E2801857530393F16D4653A5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TopLevel</td>\n",
       "      <td>9BF3403E0EB7EA8A256DA9019C0B0716</td>\n",
       "      <td>1581568955</td>\n",
       "      <td>E035DCB47CB3DF98C5CD7CFEEC3BC704</td>\n",
       "      <td>...</td>\n",
       "      <td>000012366528B5FEE179A9606DBC9826</td>\n",
       "      <td>1226</td>\n",
       "      <td>655</td>\n",
       "      <td>False</td>\n",
       "      <td>1268639592</td>\n",
       "      <td>True</td>\n",
       "      <td>1.581570e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         text_tokens hashtags  \\\n",
       "0  101\\t10117\\t140\\t119\\t142\\t119\\t152\\t119\\t1010...      NaN   \n",
       "1  101\\t10105\\t10817\\t10124\\t59232\\t18121\\t15629\\...      NaN   \n",
       "2  101\\t48561\\t10116\\t67737\\t18554\\t36371\\t10989\\...      NaN   \n",
       "3  101\\t100055\\t69940\\t10414\\t159\\t11305\\t11166\\t...      NaN   \n",
       "4  101\\t62154\\t32221\\t71843\\t10143\\t10237\\t15507\\...      NaN   \n",
       "\n",
       "                           tweet_id present_media  \\\n",
       "0  373C0F43762B7CEC1D75728BE8A33891           NaN   \n",
       "1  773A92D9E4824D06105C02BD044BB20A           NaN   \n",
       "2  218A6C27871801759F7380D7C41694A6           NaN   \n",
       "3  AB817EBA68064A0C8CBF4A6C059D92DC         Photo   \n",
       "4  349120C1E2801857530393F16D4653A5           NaN   \n",
       "\n",
       "                      present_links                   present_domains  \\\n",
       "0  A2CE3A1941BA410A1C31496C355EFCD7  E14AF8A8D257BB47587843FE7D08382B   \n",
       "1                               NaN                               NaN   \n",
       "2  5C683B5A29B308CADD0D7EFA7C9C32D3  6717B03E03DEE1D7ACAE37649ACA7BD6   \n",
       "3  E925556EE312213AD98C4D9F131D7A8D  D722330FEBEAAE68B4F4339CE8BD7C70   \n",
       "4                               NaN                               NaN   \n",
       "\n",
       "  tweet_type                          language  tweet_timestamp  \\\n",
       "0   TopLevel  D3164C7FBCF2565DDF915B1B3AEFB1DC       1582126349   \n",
       "1      Quote  D3164C7FBCF2565DDF915B1B3AEFB1DC       1581971193   \n",
       "2   TopLevel  9BF3403E0EB7EA8A256DA9019C0B0716       1582047119   \n",
       "3   TopLevel  691890251F2B9FF922BE6D3699ABEFD2       1581554925   \n",
       "4   TopLevel  9BF3403E0EB7EA8A256DA9019C0B0716       1581568955   \n",
       "\n",
       "               engaged_with_user_id  ...                  engaging_user_id  \\\n",
       "0  2A8B6AD2B9D55F535C2441AB673133D2  ...  00000865A1538142CDA5936B07FE4311   \n",
       "1  950A95B81407F33C412E520BE55A1450  ...  000009A057792FF118B9E3F2578B8407   \n",
       "2  ABB2F7F22C34057BC7B30D627B0C137A  ...  00000DEF82BE9EB5CFD07FB7DB94317B   \n",
       "3  03F96C3B7CE2179B6347AA395880C963  ...  0000109A57AFA64758EE4AAE2A01BFC7   \n",
       "4  E035DCB47CB3DF98C5CD7CFEEC3BC704  ...  000012366528B5FEE179A9606DBC9826   \n",
       "\n",
       "   engaging_user_follower_count  engaging_user_following_count  \\\n",
       "0                            65                            166   \n",
       "1                          1814                           1314   \n",
       "2                             4                             73   \n",
       "3                            15                            124   \n",
       "4                          1226                            655   \n",
       "\n",
       "   engaging_user_is_verified engaging_user_account_creation  \\\n",
       "0                      False                     1452599043   \n",
       "1                      False                     1322868747   \n",
       "2                      False                     1573996260   \n",
       "3                      False                     1385502405   \n",
       "4                      False                     1268639592   \n",
       "\n",
       "   engagee_follows_engager  reply_timestamp  retweet_timestamp  \\\n",
       "0                    False              NaN                NaN   \n",
       "1                     True     1.581979e+09                NaN   \n",
       "2                    False              NaN                NaN   \n",
       "3                     True              NaN                NaN   \n",
       "4                     True     1.581570e+09                NaN   \n",
       "\n",
       "   retweet_with_comment_timestamp  like_timestamp  \n",
       "0                             NaN             NaN  \n",
       "1                             NaN             NaN  \n",
       "2                             NaN             NaN  \n",
       "3                             NaN             NaN  \n",
       "4                             NaN             NaN  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediaCounter(row, media='Photo'):\n",
    "    counter=0\n",
    "    if type(row)==list:\n",
    "        for elem in row:\n",
    "            if elem==media:\n",
    "                counter+=1\n",
    "    else:\n",
    "        pass\n",
    "    return counter\n",
    "\n",
    "def listCounter(row):\n",
    "    counter=0\n",
    "    if type(row)==list:\n",
    "        counter+=len(row)\n",
    "    else:\n",
    "        pass\n",
    "    return counter\n",
    "\n",
    "def labelEncoder(row, mapping_encode):\n",
    "    \"\"\"\n",
    "    Label Encoding or Array<String> types\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    row : list(string)\n",
    "        List of string or labels\n",
    "    mapping_encode : dict(label, integer)\n",
    "        Encoding of some top K labels\n",
    "    Return:\n",
    "    -------\n",
    "    out : list(integers)\n",
    "        List of Label Encoders.\n",
    "        if not in mapping Encoded to len(map)\n",
    "        if not a list Encoded to len(map)+1\n",
    "    \"\"\"\n",
    "    out=[]\n",
    "    if type(row)==list:\n",
    "        for elem in row:\n",
    "            if elem in mapping_encode:\n",
    "                out.append(mapping_encode.get(elem))\n",
    "            else:\n",
    "                out.append(len(mapping_encode))\n",
    "    else:\n",
    "        out.append(len(mapping_encode)+1)\n",
    "    return out\n",
    "\n",
    "def labelEncoderSingle(row, mapping_encode):\n",
    "    out=[]\n",
    "    if row:\n",
    "        if row in mapping_encode:\n",
    "            out.append(mapping_encode.get(row))\n",
    "        else:\n",
    "            out.append(len(mapping_encode))\n",
    "    else:\n",
    "        out.append(len(mapping_encode)+1)\n",
    "    return out\n",
    "\n",
    "def hashtagSumCounter(row, mapping_hashtag_count):\n",
    "    counter=0\n",
    "    if type(row)==list:\n",
    "        for elem in row:\n",
    "            if elem in mapping_hashtag_count:\n",
    "                counter+=mapping_hashtag_count.get(elem, 0)\n",
    "    else:\n",
    "        pass\n",
    "    return counter\n",
    "\n",
    "def get_distribution_array_col(df, col):\n",
    "    distribution_df = df.select(col).filter(F.col(col).isNotNull())\\\n",
    "                              .withColumn(col, \n",
    "                                          F.explode(F.col(col)))\\\n",
    "                              .groupBy(col).count()\\\n",
    "                              .orderBy(F.col(\"count\").desc())\n",
    "    return distribution_df\n",
    "\n",
    "def save_pkl_to_s3(obj, key_filename, bucket_name):\n",
    "    serialized_obj = pickle.dumps(obj)\n",
    "    s3 = boto3.client('s3')\n",
    "    s3.put_object(Bucket=bucket_name, Key=key_filename, \n",
    "                  Body=serialized_obj)\n",
    "    \n",
    "def columns2cast(df):\n",
    "    columns = []\n",
    "    for col in df.schema:\n",
    "        if col.dataType.typeName()==\"array\":\n",
    "            columns.append(col)\n",
    "    return columns\n",
    "    \n",
    "def cast_array2string(df, columns):\n",
    "    for col in columns:\n",
    "        df = df.withColumn(col.name, F.col(col.name).cast(StringType()))\n",
    "    return df\n",
    "\n",
    "def cast_string2array(df, columns):\n",
    "    for col in columns:\n",
    "        df= df.withColumn(col, \n",
    "                          F.split(F.regexp_replace(F.col(col), r\"(^\\[)|(\\]$)|(')\", \"\"),\n",
    "                                  \", \"))\n",
    "    return df\n",
    "    \n",
    "def mappings(df, col, top_k):\n",
    "    col_dist = get_distribution_array_col(df, col)\n",
    "    df_col_dist = col_dist.limit(top_k)\n",
    "    df_col = df_col_dist.toPandas().rename(columns={'_1': col, \n",
    "                                                    '_2': 'count'})\\\n",
    "                                    .reset_index().set_index(col)\n",
    "    mapping_encode = df_col['index'].to_dict()\n",
    "    mapping_count = df_col['count'].to_dict()\n",
    "    return mapping_encode, mapping_count\n",
    "\n",
    "def mapping_label_encoder(df, col, top_k):\n",
    "    col_dist = df.select(col).filter(F.col(col).isNotNull())\\\n",
    "                      .groupBy(col).count()\\\n",
    "                      .orderBy(F.col(\"count\").desc())\n",
    "    df_col_dist = col_dist.limit(top_k)\n",
    "    df_col = df_col_dist.toPandas().rename(columns={'_1': col, \n",
    "                                                    '_2': 'count'})\\\n",
    "                                    .reset_index().set_index(col)\n",
    "    mapping_encoder = df_col['index'].to_dict()\n",
    "    return mapping_encoder\n",
    "\n",
    "def validator(df):\n",
    "    columns_w_nan = {}\n",
    "    for col in df.schema:\n",
    "        null_count = df.filter(F.col(col.name).isNull()).count()\n",
    "        if null_count>0:\n",
    "            columns_w_nan[col.name]=null_count\n",
    "    return columns_w_nan\n",
    "\n",
    "# Mappings\n",
    "tweet_type_mapping = {'TopLevel':0, 'Quote':1, 'Retweet':2, 'Reply':3}\n",
    "\n",
    "# # UDF SQL\n",
    "# PhotoCounter_udf = F.udf(lambda row: mediaCounter(row, 'Photo'), \n",
    "#                          IntegerType())\n",
    "# VideoCounter_udf = F.udf(lambda row: mediaCounter(row, 'Video'), \n",
    "#                          IntegerType())\n",
    "# GifCounter_udf = F.udf(lambda row: mediaCounter(row, 'GIF'), \n",
    "#                          IntegerType())\n",
    "# listCounter_udf = F.udf(listCounter, \n",
    "#                          IntegerType())\n",
    "# tweet_encoded_udf = F.udf(lambda x: tweet_type_mapping[x], \n",
    "#                              IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'schema'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2830e85e5da4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5271\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5272\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5273\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'schema'"
     ]
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'schema'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-41950d391333>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvalidator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcolumns_w_nan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mnull_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5271\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5272\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5273\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'schema'"
     ]
    }
   ],
   "source": [
    "df.schema\n",
    "def validator(df):\n",
    "    columns_w_nan = {}\n",
    "    for col in df.schema:\n",
    "        null_count = df.filter(F.col(col.name).isNull()).count()\n",
    "        if null_count>0:\n",
    "            columns_w_nan[col.name]=null_count\n",
    "    return columns_w_nan\n",
    "columns_w_nan = validator(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import boto3\n",
    "import gc\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (FloatType, DateType, StructType, StructField, StringType, LongType, \n",
    "    IntegerType, ArrayType, BooleanType, DoubleType)\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, QuantileDiscretizer\n",
    "gc.enable()\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.sql.shuffle.partitions\", 1000).appName(\"twitter\").getOrCreate()\n",
    "print(spark.sparkContext.getConf().get('spark.driver.memory'))\n",
    "print(spark.sparkContext.getConf().get(\"spark.sql.shuffle.partitions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(path='training.tsv', has_labels=True, schema='auto'):\n",
    "    \"\"\"\n",
    "    Parses the training data for the Twitter RecSys Challenge.\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder.appName(\"twitter\").getOrCreate()\n",
    "    if schema == 'auto':\n",
    "        schema = build_schema(has_labels)\n",
    "    df = spark.read.csv(path, schema=schema, sep='\\x01', encoding='utf-8',\n",
    "                        ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True)\n",
    "    df = df.withColumn('text_tokens', F.split('text_tokens', '\\t'))\n",
    "    df = df.withColumn('hashtags', F.split('hashtags', '\\t'))\n",
    "    df = df.withColumn('present_media', F.split('present_media', '\\t'))\n",
    "    df = df.withColumn('present_links', F.split('present_links', '\\t'))\n",
    "    df = df.withColumn('present_domains', F.split('present_domains', '\\t'))\n",
    "    return df\n",
    "\n",
    "def build_schema(has_labels=True):\n",
    "    if has_labels:\n",
    "        schema = StructType([StructField('text_tokens', StringType()),\n",
    "                             StructField('hashtags', StringType()),\n",
    "                             StructField('tweet_id', StringType()),\n",
    "                             StructField('present_media', StringType()),\n",
    "                             StructField('present_links', StringType()),\n",
    "                             StructField('present_domains', StringType()),\n",
    "                             StructField('tweet_type', StringType()),\n",
    "                             StructField('language', StringType()),\n",
    "                             StructField('tweet_timestamp', LongType()),\n",
    "                             StructField('engaged_with_user_id', StringType()),\n",
    "                             StructField('engaged_with_user_follower_count', IntegerType()),\n",
    "                             StructField('engaged_with_user_following_count', IntegerType()),\n",
    "                             StructField('engaged_with_user_is_verified', BooleanType()),\n",
    "                             StructField('engaged_with_user_account_creation', LongType()),\n",
    "                             StructField('engaging_user_id', StringType()),\n",
    "                             StructField('engaging_user_follower_count', IntegerType()),\n",
    "                             StructField('engaging_user_following_count', IntegerType()),\n",
    "                             StructField('engaging_user_is_verified', BooleanType()),\n",
    "                             StructField('engaging_user_account_creation', LongType()),\n",
    "                             StructField('engagee_follows_engager', BooleanType()),\n",
    "                             StructField('reply_timestamp', LongType()),\n",
    "                             StructField('retweet_timestamp', LongType()),\n",
    "                             StructField('retweet_with_comment_timestamp', LongType()),\n",
    "                             StructField('like_timestamp', LongType())\n",
    "                            ])\n",
    "    else:\n",
    "         schema = StructType([StructField('text_tokens', StringType()),\n",
    "                             StructField('hashtags', StringType()),\n",
    "                             StructField('tweet_id', StringType()),\n",
    "                             StructField('present_media', StringType()),\n",
    "                             StructField('present_links', StringType()),\n",
    "                             StructField('present_domains', StringType()),\n",
    "                             StructField('tweet_type', StringType()),\n",
    "                             StructField('language', StringType()),\n",
    "                             StructField('tweet_timestamp', LongType()),\n",
    "                             StructField('engaged_with_user_id', StringType()),\n",
    "                             StructField('engaged_with_user_follower_count', IntegerType()),\n",
    "                             StructField('engaged_with_user_following_count', IntegerType()),\n",
    "                             StructField('engaged_with_user_is_verified', BooleanType()),\n",
    "                             StructField('engaged_with_user_account_creation', LongType()),\n",
    "                             StructField('engaging_user_id', StringType()),\n",
    "                             StructField('engaging_user_follower_count', IntegerType()),\n",
    "                             StructField('engaging_user_following_count', IntegerType()),\n",
    "                             StructField('engaging_user_is_verified', BooleanType()),\n",
    "                             StructField('engaging_user_account_creation', LongType()),\n",
    "                             StructField('engagee_follows_engager', BooleanType())\n",
    "                            ])\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediaCounter(row, media='Photo'):\n",
    "    counter=0\n",
    "    if type(row)==list:\n",
    "        for elem in row:\n",
    "            if elem==media:\n",
    "                counter+=1\n",
    "    else:\n",
    "        pass\n",
    "    return counter\n",
    "\n",
    "def listCounter(row):\n",
    "    counter=0\n",
    "    if type(row)==list:\n",
    "        counter+=len(row)\n",
    "    else:\n",
    "        pass\n",
    "    return counter\n",
    "\n",
    "def labelEncoder(row, mapping_encode):\n",
    "    \"\"\"\n",
    "    Label Encoding or Array<String> types\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    row : list(string)\n",
    "        List of string or labels\n",
    "    mapping_encode : dict(label, integer)\n",
    "        Encoding of some top K labels\n",
    "    Return:\n",
    "    -------\n",
    "    out : list(integers)\n",
    "        List of Label Encoders.\n",
    "        if not in mapping Encoded to len(map)\n",
    "        if not a list Encoded to len(map)+1\n",
    "    \"\"\"\n",
    "    out=[]\n",
    "    if type(row)==list:\n",
    "        for elem in row:\n",
    "            if elem in mapping_encode:\n",
    "                out.append(mapping_encode.get(elem))\n",
    "            else:\n",
    "                out.append(len(mapping_encode))\n",
    "    else:\n",
    "        out.append(len(mapping_encode)+1)\n",
    "    return out\n",
    "\n",
    "def labelEncoderSingle(row, mapping_encode):\n",
    "    out=[]\n",
    "    if row:\n",
    "        if row in mapping_encode:\n",
    "            out.append(mapping_encode.get(row))\n",
    "        else:\n",
    "            out.append(len(mapping_encode))\n",
    "    else:\n",
    "        out.append(len(mapping_encode)+1)\n",
    "    return out\n",
    "\n",
    "def hashtagSumCounter(row, mapping_hashtag_count):\n",
    "    counter=0\n",
    "    if type(row)==list:\n",
    "        for elem in row:\n",
    "            if elem in mapping_hashtag_count:\n",
    "                counter+=mapping_hashtag_count.get(elem, 0)\n",
    "    else:\n",
    "        pass\n",
    "    return counter\n",
    "\n",
    "def get_distribution_array_col(df, col):\n",
    "    distribution_df = df.select(col).filter(F.col(col).isNotNull())\\\n",
    "                              .withColumn(col, \n",
    "                                          F.explode(F.col(col)))\\\n",
    "                              .groupBy(col).count()\\\n",
    "                              .orderBy(F.col(\"count\").desc())\n",
    "    return distribution_df\n",
    "\n",
    "def save_pkl_to_s3(obj, key_filename, bucket_name):\n",
    "    serialized_obj = pickle.dumps(obj)\n",
    "    s3 = boto3.client('s3')\n",
    "    s3.put_object(Bucket=bucket_name, Key=key_filename, \n",
    "                  Body=serialized_obj)\n",
    "    \n",
    "def columns2cast(df):\n",
    "    columns = []\n",
    "    for col in df.schema:\n",
    "        if col.dataType.typeName()==\"array\":\n",
    "            columns.append(col)\n",
    "    return columns\n",
    "    \n",
    "def cast_array2string(df, columns):\n",
    "    for col in columns:\n",
    "        df = df.withColumn(col.name, F.col(col.name).cast(StringType()))\n",
    "    return df\n",
    "\n",
    "def cast_string2array(df, columns):\n",
    "    for col in columns:\n",
    "        df= df.withColumn(col, \n",
    "                          F.split(F.regexp_replace(F.col(col), r\"(^\\[)|(\\]$)|(')\", \"\"),\n",
    "                                  \", \"))\n",
    "    return df\n",
    "    \n",
    "def mappings(df, col, top_k):\n",
    "    col_dist = get_distribution_array_col(df, col)\n",
    "    df_col_dist = col_dist.limit(top_k)\n",
    "    df_col = df_col_dist.toPandas().rename(columns={'_1': col, \n",
    "                                                    '_2': 'count'})\\\n",
    "                                    .reset_index().set_index(col)\n",
    "    mapping_encode = df_col['index'].to_dict()\n",
    "    mapping_count = df_col['count'].to_dict()\n",
    "    return mapping_encode, mapping_count\n",
    "\n",
    "def mapping_label_encoder(df, col, top_k):\n",
    "    col_dist = df.select(col).filter(F.col(col).isNotNull())\\\n",
    "                      .groupBy(col).count()\\\n",
    "                      .orderBy(F.col(\"count\").desc())\n",
    "    df_col_dist = col_dist.limit(top_k)\n",
    "    df_col = df_col_dist.toPandas().rename(columns={'_1': col, \n",
    "                                                    '_2': 'count'})\\\n",
    "                                    .reset_index().set_index(col)\n",
    "    mapping_encoder = df_col['index'].to_dict()\n",
    "    return mapping_encoder\n",
    "\n",
    "def validator(df):\n",
    "    columns_w_nan = {}\n",
    "    for col in df.schema:\n",
    "        null_count = df.filter(F.col(col.name).isNull()).count()\n",
    "        if null_count>0:\n",
    "            columns_w_nan[col.name]=null_count\n",
    "    return columns_w_nan\n",
    "\n",
    "# Mappings\n",
    "tweet_type_mapping = {'TopLevel':0, 'Quote':1, 'Retweet':2, 'Reply':3}\n",
    "\n",
    "# UDF SQL\n",
    "PhotoCounter_udf = F.udf(lambda row: mediaCounter(row, 'Photo'), \n",
    "                         IntegerType())\n",
    "VideoCounter_udf = F.udf(lambda row: mediaCounter(row, 'Video'), \n",
    "                         IntegerType())\n",
    "GifCounter_udf = F.udf(lambda row: mediaCounter(row, 'GIF'), \n",
    "                         IntegerType())\n",
    "listCounter_udf = F.udf(listCounter, \n",
    "                         IntegerType())\n",
    "tweet_encoded_udf = F.udf(lambda x: tweet_type_mapping[x], \n",
    "                             IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_size={\"final-complete\": {\"val_size\": 500000, \n",
    "                                    \"train_size\": \"all\"}}\n",
    "\n",
    "training = False\n",
    "submission = False\n",
    "test = True\n",
    "\n",
    "bucket='bucket-name'\n",
    "s3_resource = boto3.resource('s3')\n",
    "top_k_languages = 30\n",
    "top_k_domains = 3000\n",
    "top_k_hashtags = 13000\n",
    "\n",
    "# Embeddings\n",
    "num_partitions=1000\n",
    "\n",
    "# Buckets\n",
    "partition_per_cluster = 100\n",
    "\n",
    "suffix_sample = \"final-complete\" #\"full\", \"small\", \"medium\", \"sub_medium\"\n",
    "data_path = \"final-data\"\n",
    "object_paths = \"final-artifacts\"\n",
    "\n",
    "val_size = dictionary_size[suffix_sample][\"val_size\"]\n",
    "train_size = dictionary_size[suffix_sample][\"train_size\"]\n",
    "\n",
    "bucket_s3 = s3_resource.Bucket(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S3\n",
    "twitter_bucket_s3 = \"../dataset/twitter/\"\n",
    "trainining_path = os.path.join(twitter_bucket_s3, \"train100K.csv\")\n",
    "submission_path = os.path.join(twitter_bucket_s3, \"train100K.csv\")\n",
    "test_path = os.path.join(twitter_bucket_s3, \"train100K.csv\")\n",
    "\n",
    "# Splitted paths\n",
    "train_path = os.path.join(twitter_bucket_s3, data_path, \"train-\"+suffix_sample)\n",
    "val_path = os.path.join(twitter_bucket_s3, data_path, \"val-\"+suffix_sample)\n",
    "\n",
    "# Processed\n",
    "processed_train_path = os.path.join(twitter_bucket_s3, data_path, \"processed\", \"train-\"+suffix_sample)\n",
    "processed_val_path = os.path.join(twitter_bucket_s3, data_path, \"processed\", \"val-\"+suffix_sample)\n",
    "processed_submission_path = os.path.join(twitter_bucket_s3, data_path, \"processed\", \"submission-\"+suffix_sample)\n",
    "processed_test_path = os.path.join(twitter_bucket_s3, data_path, \"processed\", \"test-\"+suffix_sample)\n",
    "processed_emb_train_path = os.path.join(twitter_bucket_s3, data_path, \"processed-embeddings-final\", \n",
    "                                        \"train-\"+suffix_sample)\n",
    "processed_emb_val_path = os.path.join(twitter_bucket_s3, data_path, \"processed-embeddings-final\", \n",
    "                                      \"val-\"+suffix_sample)\n",
    "processed_emb_submission_path = os.path.join(twitter_bucket_s3, data_path, \"processed-embeddings-final\", \n",
    "                                         \"submission-\"+suffix_sample)\n",
    "processed_emb_test_path = os.path.join(twitter_bucket_s3, data_path, \"processed-embeddings-final\", \n",
    "                                         \"test-\"+suffix_sample)\n",
    "processed_top_train_path = os.path.join(twitter_bucket_s3, data_path, \"processed-topics\", \n",
    "                                        \"train-\"+suffix_sample)\n",
    "processed_top_val_path = os.path.join(twitter_bucket_s3, data_path, \"processed-topics\", \n",
    "                                      \"val-\"+suffix_sample)\n",
    "processed_top_submission_path = os.path.join(twitter_bucket_s3, data_path, \"processed-topics\", \n",
    "                                             \"submission-\"+suffix_sample)\n",
    "processed_top_test_path = os.path.join(twitter_bucket_s3, data_path, \"processed-topics\", \n",
    "                                             \"test-\"+suffix_sample)\n",
    "# Resources\n",
    "engaging_users_training_path = os.path.join(twitter_bucket_s3, data_path, \"engaging-users-training\")\n",
    "engaging_users_training_path = os.path.join(twitter_bucket_s3, \"engaging-users-training\")\n",
    "\n",
    "engaging_users_submission_path = os.path.join(twitter_bucket_s3, data_path, \"engaging-users-submission\")\n",
    "engaging_users_submission_path = os.path.join(twitter_bucket_s3, \"engaging-users-submission\")\n",
    "\n",
    "engaging_users_test_path = os.path.join(twitter_bucket_s3, data_path, \"engaging-users-test\")\n",
    "intentions_path = os.path.join(twitter_bucket_s3, data_path, \"intentions-\"+suffix_sample)\n",
    "map_user_bucket_path = os.path.join(twitter_bucket_s3, data_path, \"map_user_bucket\")\n",
    "\n",
    "topic_encodings_path = os.path.join(twitter_bucket_s3, \"data\", \"textEncodings\", \"user_topics\")\n",
    "users_intime_path = os.path.join(twitter_bucket_s3, data_path, \"users_intime-\"+suffix_sample)\n",
    "\n",
    "# keys objects\n",
    "key_hashtag_mapping = os.path.join(object_paths, f'hashtag_mapping_{suffix_sample}.pkl')\n",
    "key_domain_mapping = os.path.join(object_paths, f'domain_mapping_{suffix_sample}.pkl')\n",
    "key_language_mapping = os.path.join(object_paths, f'language_mapping_{suffix_sample}.pkl')\n",
    "key_hashtag_count = os.path.join(object_paths, f'hashtag_count_{suffix_sample}.pkl')\n",
    "key_domain_count = os.path.join(object_paths, f'domain_count_{suffix_sample}.pkl')\n",
    "key_scaling_features = os.path.join(object_paths, f'scaling_dictionary_{suffix_sample}.pkl')\n",
    "key_diff_min = os.path.join(object_paths, f'diff_min_{suffix_sample}.pkl')\n",
    "key_impute_perc = os.path.join(object_paths, f'dict_mean_perc_{suffix_sample}.pkl')\n",
    "key_topiccount = os.path.join(object_paths, f'topiccount_{suffix_sample}.pkl')\n",
    "\n",
    "# s3+keys\n",
    "columns = [\"engaged_with_user_follower_count\", \"engaged_with_user_following_count\",\n",
    "           \"engaged_with_user_account_creation\", \"engaging_user_follower_count\",\n",
    "           \"engaging_user_following_count\", \"engaging_user_account_creation\"]\n",
    "qds_paths = {}\n",
    "for col in columns:\n",
    "    qds_paths[col] = os.path.join(twitter_bucket_s3, object_paths, f\"qs_{suffix_sample}_\" + col)\n",
    "    \n",
    "# Bucket pipeline\n",
    "users_buckets = os.path.join(twitter_bucket_s3, data_path, \"users_buckets\") #\n",
    "users_buckets_part_2 = os.path.join(twitter_bucket_s3, data_path, \"users_buckets_part_2\") #\n",
    "\n",
    "pipeline_kmeans_path = os.path.join(twitter_bucket_s3, object_paths, \"pipeline_id_encoding\")\n",
    "cluster_map_path = os.path.join(twitter_bucket_s3, data_path, \"cluster_map\")\n",
    "\n",
    "# Embeddings\n",
    "bert_embeddings_train = os.path.join(twitter_bucket_s3, \"data\", \"textEncodings\", \"tweets_extended\")\n",
    "submission_rawTweetEncodings_path = os.path.join(twitter_bucket_s3, \"data\", \"textEncodings\", \"submission-tweets-extended\")\n",
    "test_rawTweetEncodings_path = os.path.join(twitter_bucket_s3, \"data\", \"textEncodings\", \"test-tweets-extended\")\n",
    "\n",
    "# Topics pipeline\n",
    "reduced_topics_path = os.path.join(twitter_bucket_s3, \"data\", \"textEncodings\", \"reducedTopics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "df = parse_data(trainining_path, has_labels=True).repartition(500)\n",
    "engaging_user_id_train = df.select(\"engaging_user_id\").distinct()\n",
    "engaging_user_id_train.write.csv(engaging_users_training_path)\n",
    "\n",
    "engaging_users_train = spark.read.csv(engaging_users_training_path, \n",
    "                                      schema=StructType([StructField('engaging_user_id', StringType())]))\n",
    "\n",
    "# Submission\n",
    "df = parse_data(submission_path, has_labels=False).repartition(200)\n",
    "engaging_users_submission = df.select(\"engaging_user_id\").distinct()\n",
    "engaging_users_submission.write.csv(engaging_users_submission_path)\n",
    "\n",
    "engaging_users_submission = spark.read.csv(engaging_users_submission_path, \n",
    "                                    schema=StructType([StructField('engaging_user_id', StringType())]))\n",
    "\n",
    "# Test\n",
    "df = parse_data(test_path, has_labels=False).repartition(200)\n",
    "engaging_users_test = df.select(\"engaging_user_id\").distinct()\n",
    "engaging_users_test.write.csv(engaging_users_test_path)\n",
    "\n",
    "engaging_users_test = spark.read.csv(engaging_users_test_path, \n",
    "                                    schema=StructType([StructField('engaging_user_id', StringType())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_s3.objects.filter(Prefix=f\"{data_path}/engaging-users-training\", Delimiter='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../dataset/twitter/engaging-users-training'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engaging_users_training_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[engaging_user_id: string]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engaging_users_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[engaging_user_id: string]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engaging_user_id_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_data(trainining_path, has_labels=True).repartition(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[text_tokens: array<string>, hashtags: array<string>, tweet_id: string, present_media: array<string>, present_links: array<string>, present_domains: array<string>, tweet_type: string, language: string, tweet_timestamp: bigint, engaged_with_user_id: string, engaged_with_user_follower_count: int, engaged_with_user_following_count: int, engaged_with_user_is_verified: boolean, engaged_with_user_account_creation: bigint, engaging_user_id: string, engaging_user_follower_count: int, engaging_user_following_count: int, engaging_user_is_verified: boolean, engaging_user_account_creation: bigint, engagee_follows_engager: boolean, reply_timestamp: bigint, retweet_timestamp: bigint, retweet_with_comment_timestamp: bigint, like_timestamp: bigint]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_final_df(df, time_holdout_fraction=0.1, space_column='engaging_user_id', \n",
    "                   time_column='tweet_timestamp', avg_rows_per_user=5.4, \n",
    "                   val_size = 500000, perc_val_not_shared = 0.27, seed=0, \n",
    "                   engaging_users_submission=None, engaging_users_test=None):\n",
    "    \"\"\"\n",
    "    Split using all the train data.\n",
    "    Exclude all engaging_users from submission and test to create the validation\n",
    "    Select the validatioon to be up to 500k samples, where 23% of enagging users are not known \n",
    "    (out of space completely). The validation is going to be picked out of time, but the rest \n",
    "    of out of time is gooing ot be used in train\n",
    "    \"\"\"\n",
    "    info_dict = {}\n",
    "    min_date, max_date = df.select(F.min(time_column), F.max(time_column)).first()\n",
    "    print(df.select(F.min(time_column)))\n",
    "    print(type(F.max(time_column).first()))\n",
    "    time_range = max_date - min_date\n",
    "    time_holdout_timestamp = min_date + int(time_range*(1-time_holdout_fraction))\n",
    "\n",
    "    #Get engaging_users on submission and test\n",
    "    engaging_users_test = engaging_users_submission.union(engaging_users_test)\n",
    "    engaging_users_test = engaging_users_test.select(F.col(\"engaging_user_id\").alias(\"user_id_1\")).distinct()\n",
    "\n",
    "    join_users_df = engaging_users_train.join(engaging_users_test, \n",
    "                                              engaging_users_test.user_id_1==engaging_users_train.engaging_user_id, \n",
    "                                              how=\"left\")\n",
    "    join_users_df = join_users_df.withColumn(\"indicator_test\", \n",
    "                                             F.when(F.col(\"user_id_1\").isNotNull(), 1).otherwise(0))\n",
    "    join_users_df = join_users_df.drop(\"user_id_1\") # Training user w indicator also in test\n",
    "    join_users_not_test = join_users_df.filter(F.col(\"indicator_test\")==0)\n",
    "    \n",
    "    # Sample that are not in test\n",
    "    df_not_test = df.join(join_users_not_test,\n",
    "                          on=space_column, \n",
    "                          how=\"inner\")\n",
    "    df_not_test = df_not_test.drop(\"indicator_test\")\n",
    "    info_dict[\"df_not_test_count\"] = df_not_test.count()\n",
    "\n",
    "    df_not_test_intime = df_not_test.filter(F.col(time_column) <= time_holdout_timestamp)\n",
    "    df_not_test_outtime = df_not_test.filter(F.col(time_column) > time_holdout_timestamp) # Set for validation\n",
    "    info_dict[\"df_not_test_intime_count\"] = df_not_test_intime.count() #\n",
    "    info_dict[\"df_not_test_outtime_count\"] = df_not_test_outtime.count() #\n",
    "    engaging_user_id_intime_not_test = df_not_test_intime.select(F.col(space_column)).distinct()\n",
    "    engaging_user_id_outtime_not_test = df_not_test_outtime.select(F.col(space_column)).distinct()\n",
    "    info_dict[\"engaging_user_id_intime_not_test_count\"] = engaging_user_id_intime_not_test.count() #\n",
    "    info_dict[\"engaging_user_id_outtime_not_test_count\"] = engaging_user_id_outtime_not_test.count() #\n",
    "    inner_engaging_not_test = engaging_user_id_intime_not_test.join(engaging_user_id_outtime_not_test, \n",
    "                                                                    on=space_column, \n",
    "                                                                    how=\"inner\")\n",
    "    info_dict[\"inner_engaging_not_test_count\"] = inner_engaging_not_test.count() #\n",
    "    rows_per_user_outtime_not_test = info_dict[\"df_not_test_outtime_count\"]/\\\n",
    "                                      info_dict[\"engaging_user_id_outtime_not_test_count\"]\n",
    "    \n",
    "    engaging_user_id_only_outtime_not_test =engaging_user_id_outtime_not_test.join(inner_engaging_not_test,\n",
    "                                                                                   on=space_column,\n",
    "                                                                                   how=\"left_anti\")\n",
    "    info_dict[\"engaging_user_id_only_outtime_not_test_count\"] = engaging_user_id_only_outtime_not_test.count()#\n",
    "    frac_user_only_outtime = val_size/rows_per_user_outtime_not_test*(perc_val_not_shared)\\\n",
    "                        /info_dict[\"engaging_user_id_only_outtime_not_test_count\"] # Validation shared \n",
    "    valid_users_not_test_outtime = engaging_user_id_only_outtime_not_test.sample(withReplacement=False,\n",
    "                                                                           fraction=frac_user_only_outtime,\n",
    "                                                                           seed=seed)\n",
    "    valid_users_not_test_outtime = valid_users_not_test_outtime.select(F.col(\"engaging_user_id\").alias(\"user_id\"))\n",
    "\n",
    "    # Select the sample from that correspnd to 27% of rows\n",
    "    df_valid_unknown = df_not_test_outtime.join(valid_users_not_test_outtime, \n",
    "                                    df_not_test_outtime.engaging_user_id==valid_users_not_test_outtime.user_id,\n",
    "                                    how=\"inner\").drop(\"user_id\")\n",
    "    \n",
    "    #Select the rest\n",
    "    frac_user_inner = val_size/rows_per_user_outtime_not_test*(1-perc_val_not_shared)\\\n",
    "                        /info_dict[\"inner_engaging_not_test_count\"] # Validation shared \n",
    "    inner_engaging_not_test = inner_engaging_not_test.select(F.col(\"engaging_user_id\").alias(\"user_id\"))\n",
    "    valid_users_not_test_shared = inner_engaging_not_test.sample(withReplacement=False,\n",
    "                                                                 fraction=frac_user_inner,\n",
    "                                                                 seed=seed)\n",
    "    df_valid_known = df_not_test_outtime.join(valid_users_not_test_shared,\n",
    "                                    df_not_test_outtime.engaging_user_id==inner_engaging_not_test.user_id,\n",
    "                                    how=\"inner\").drop(\"user_id\")\n",
    "    df_valid = df_valid_known.union(df_valid_unknown)\n",
    "\n",
    "    valid_samples = df_valid.select(F.col(\"tweet_id\").alias(\"tweet_id_1\"), \n",
    "                                    F.col(\"engaging_user_id\").alias(\"engaging_user_id_1\"))\n",
    "    df_train = df.join(valid_samples, \n",
    "                       (df.tweet_id==valid_samples.tweet_id_1)&\\\n",
    "                       (df.engaging_user_id == valid_samples.engaging_user_id_1), \n",
    "                       how=\"left_anti\")\n",
    "    return df_train, df_valid, join_users_not_test, info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[min(tweet_timestamp): bigint]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-7d851fc36d43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m            \u001b[0mperc_val_not_shared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.27\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m            \u001b[0mengaging_users_submission\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengaging_users_submission\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m            engaging_users_test=engaging_users_test)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"info_dict: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-2b1532466ed0>\u001b[0m in \u001b[0;36msplit_final_df\u001b[0;34m(df, time_holdout_fraction, space_column, time_column, avg_rows_per_user, val_size, perc_val_not_shared, seed, engaging_users_submission, engaging_users_test)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmin_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mtime_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_date\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin_date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtime_holdout_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_date\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_range\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtime_holdout_fraction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "df_train, df_valid, join_users_not_test, info_dict = split_final_df(df, time_holdout_fraction=0.1, \n",
    "           space_column='engaging_user_id', \n",
    "           time_column='tweet_timestamp',\n",
    "           avg_rows_per_user=5.4, val_size=val_size, \n",
    "           perc_val_not_shared=0.27, seed=0,\n",
    "           engaging_users_submission=engaging_users_submission, \n",
    "           engaging_users_test=engaging_users_test)\n",
    "print(\"info_dict: \", info_dict)\n",
    "train = df_train\n",
    "val = df_valid\n",
    "columns = columns2cast(train)\n",
    "train = cast_array2string(train, columns)\n",
    "val = cast_array2string(val, columns)\n",
    "join_users_not_test.write.csv(users_intime_path)\n",
    "train.write.csv(train_path)\n",
    "val.repartition(1000).write.csv(val_path)\n",
    "print(\"Casted columns: \",columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('tweetEncoded', tweet_encoded_udf(df.tweet_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('linkCount', listCounter_udf(df.present_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('text_tokens', StringType()),\n",
    "                     StructField('hashtags', StringType()),\n",
    "                     StructField('tweet_id', StringType()),\n",
    "                     StructField('present_media', StringType()),\n",
    "                     StructField('present_links', StringType()),\n",
    "                     StructField('present_domains', StringType()),\n",
    "                     StructField('tweet_type', StringType()),\n",
    "                     StructField('language', StringType()),\n",
    "                     StructField('tweet_timestamp', LongType()),\n",
    "                     StructField('engaged_with_user_id', StringType()),\n",
    "                     StructField('engaged_with_user_follower_count', IntegerType()),\n",
    "                     StructField('engaged_with_user_following_count', IntegerType()),\n",
    "                     StructField('engaged_with_user_is_verified', BooleanType()),\n",
    "                     StructField('engaged_with_user_account_creation', LongType()),\n",
    "                     StructField('engaging_user_id', StringType()),\n",
    "                     StructField('engaging_user_follower_count', IntegerType()),\n",
    "                     StructField('engaging_user_following_count', IntegerType()),\n",
    "                     StructField('engaging_user_is_verified', BooleanType()),\n",
    "                     StructField('engaging_user_account_creation', LongType()),\n",
    "                     StructField('engagee_follows_engager', BooleanType()),\n",
    "                     StructField('reply_timestamp', LongType()),\n",
    "                     StructField('retweet_timestamp', LongType()),\n",
    "                     StructField('retweet_with_comment_timestamp', LongType()),\n",
    "                     StructField('like_timestamp', LongType())])\n",
    "train = spark.read.csv(trainining_path, schema=schema)\n",
    "schema = StructType([StructField('engaging_user_id', StringType()),\n",
    "                     StructField('text_tokens', StringType()),\n",
    "                     StructField('hashtags', StringType()),\n",
    "                     StructField('tweet_id', StringType()),\n",
    "                     StructField('present_media', StringType()),\n",
    "                     StructField('present_links', StringType()),\n",
    "                     StructField('present_domains', StringType()),\n",
    "                     StructField('tweet_type', StringType()),\n",
    "                     StructField('language', StringType()),\n",
    "                     StructField('tweet_timestamp', LongType()),\n",
    "                     StructField('engaged_with_user_id', StringType()),\n",
    "                     StructField('engaged_with_user_follower_count', IntegerType()),\n",
    "                     StructField('engaged_with_user_following_count', IntegerType()),\n",
    "                     StructField('engaged_with_user_is_verified', BooleanType()),\n",
    "                     StructField('engaged_with_user_account_creation', LongType()),\n",
    "                     StructField('engaging_user_follower_count', IntegerType()),\n",
    "                     StructField('engaging_user_following_count', IntegerType()),\n",
    "                     StructField('engaging_user_is_verified', BooleanType()),\n",
    "                     StructField('engaging_user_account_creation', LongType()),\n",
    "                     StructField('engagee_follows_engager', BooleanType()),\n",
    "                     StructField('reply_timestamp', LongType()),\n",
    "                     StructField('retweet_timestamp', LongType()),\n",
    "                     StructField('retweet_with_comment_timestamp', LongType()),\n",
    "                     StructField('like_timestamp', LongType())])\n",
    "val = spark.read.csv(trainining_path, schema=schema).repartition(1000)\n",
    "join_users_not_test = spark.read.csv(trainining_path, \n",
    "                              schema=StructType([StructField('engaging_user_id', StringType())]))\n",
    "columns = [\"text_tokens\", \"hashtags\", \"present_media\", \"present_links\", \"present_domains\"]\n",
    "train = cast_string2array(train, columns)\n",
    "val = cast_string2array(val, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../dataset/twitter/final-data/train-final-complete'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('indicator_reply',F.when(F.col('reply_timestamp').isNotNull(), 1).otherwise(0))\n",
    "df = df.withColumn('indicator_retweet',F.when(F.col('retweet_timestamp').isNotNull(), 1).otherwise(0))\n",
    "df = df.withColumn('indicator_retweet_with_comment',\n",
    "                   F.when(F.col('retweet_with_comment_timestamp').isNotNull(),1).otherwise(0))\n",
    "df = df.withColumn('indicator_like', F.when(F.col('like_timestamp').isNotNull(),1).otherwise(0))\n",
    "df = df.withColumn('indicator_interaction', \n",
    "                   F.when(F.col('indicator_reply')+\\\n",
    "                          F.col('indicator_retweet')+\\\n",
    "                          F.col('indicator_retweet_with_comment')+\\\n",
    "                          F.col('indicator_like')>0, 1)\\\n",
    "                   .otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "intention_df = df.select(\"engaging_user_id\", \"indicator_reply\", \"indicator_retweet\", \n",
    "                                    \"indicator_retweet_with_comment\", \"indicator_like\", \"indicator_interaction\")\\\n",
    "                        .groupBy(\"engaging_user_id\").agg(F.sum(F.col(\"indicator_interaction\")).alias(\"n_interactions\"), \n",
    "                                                         F.sum(F.col(\"indicator_retweet_with_comment\"))\\\n",
    "                                                         .alias(\"n_commented\"),\n",
    "                                                         F.sum(F.col(\"indicator_like\")).alias(\"n_liked\"),\n",
    "                                                         F.sum(F.col(\"indicator_reply\")).alias(\"n_replied\"),\n",
    "                                                         F.sum(F.col(\"indicator_retweet\")).alias(\"n_retweeted\"),\n",
    "                                                         F.count(F.col(\"indicator_interaction\"))\\\n",
    "                                                         .alias(\"total_appearance\"))\n",
    "columns = ['n_interactions', 'n_commented', 'n_liked', 'n_replied', 'n_retweeted']\n",
    "for col_i in columns:\n",
    "    intention_df = intention_df.withColumn(\"perc_\" + col_i, F.col(col_i)/(F.col(\"total_appearance\")))\n",
    "intention_df = intention_df.drop(*columns)\n",
    "join_users_not_test = join_users_not_test.select(F.col(\"engaging_user_id\").alias(\"drop_users\"))\n",
    "join_users_not_test = join_users_not_test.sample(withReplacement=False,\n",
    "                                                 fraction=0.15,\n",
    "                                                 seed=42)\n",
    "intention_df = intention_df.join(join_users_not_test, \n",
    "                                 intention_df.engaging_user_id==join_users_not_test.drop_users, \n",
    "                                 how=\"left_anti\").drop(\"drop_users\")\n",
    "intention_df.repartition(1000).write.csv(intentions_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoCredentialsError",
     "evalue": "Unable to locate credentials",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoCredentialsError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-56a03fd41df3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmapping_hashtag_encode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping_hashtag_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmappings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hashtags\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k_hashtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Saving pkl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msave_pkl_to_s3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapping_hashtag_encode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_hashtag_mapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msave_pkl_to_s3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapping_hashtag_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_hashtag_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-9a2147d61a60>\u001b[0m in \u001b[0;36msave_pkl_to_s3\u001b[0;34m(obj, key_filename, bucket_name)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0ms3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     s3.put_object(Bucket=bucket_name, Key=key_filename, \n\u001b[0;32m---> 81\u001b[0;31m                   Body=serialized_obj)\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcolumns2cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    336\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m             http, parsed_response = self._make_request(\n\u001b[0;32m--> 643\u001b[0;31m                 operation_model, request_dict, request_context)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         self.meta.events.emit(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, operation_model, request_dict, request_context)\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_endpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m             self.meta.events.emit(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36mmake_request\u001b[0;34m(self, operation_model, request_dict)\u001b[0m\n\u001b[1;32m    100\u001b[0m         logger.debug(\"Making request for %s with params: %s\",\n\u001b[1;32m    101\u001b[0m                      operation_model, request_dict)\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, request_dict, operation_model)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mattempts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'context'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         success_response, exception = self._get_response(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36mcreate_request\u001b[0;34m(self, params, operation_model)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 op_name=operation_model.name)\n\u001b[1;32m    115\u001b[0m             self._event_emitter.emit(event_name, request=request,\n\u001b[0;32m--> 116\u001b[0;31m                                      operation_name=operation_model.name)\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mprepared_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprepared_request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/botocore/hooks.py\u001b[0m in \u001b[0;36memit\u001b[0;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0memit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0maliased_event_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_event_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_emitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maliased_event_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0memit_until_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/botocore/hooks.py\u001b[0m in \u001b[0;36memit\u001b[0;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m                  \u001b[0mhandlers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \"\"\"\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_emit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0memit_until_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/botocore/hooks.py\u001b[0m in \u001b[0;36m_emit\u001b[0;34m(self, event_name, kwargs, stop_on_response)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers_to_call\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Event %s: calling handler %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m             \u001b[0mresponses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_on_response\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/botocore/signers.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(self, operation_name, request, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# this method is invoked to sign the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# Don't call this method directly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     def sign(self, operation_name, request, region_name=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/botocore/signers.py\u001b[0m in \u001b[0;36msign\u001b[0;34m(self, operation_name, request, region_name, signing_type, expires_in, signing_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_choose_signer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigning_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/botocore/auth.py\u001b[0m in \u001b[0;36madd_auth\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNoCredentialsError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0mdatetime_now\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutcnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime_now\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSIGV4_TIMESTAMP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoCredentialsError\u001b[0m: Unable to locate credentials"
     ]
    }
   ],
   "source": [
    "mapping_hashtag_encode, mapping_hashtag_count = mappings(df, \"hashtags\", top_k_hashtags)\n",
    "# Saving pkl\n",
    "save_pkl_to_s3(mapping_hashtag_encode, key_hashtag_mapping, bucket)\n",
    "save_pkl_to_s3(mapping_hashtag_count, key_hashtag_count, bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_hashtag_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, {})"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mappings(df, \"hashtags\", top_k_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13000"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
