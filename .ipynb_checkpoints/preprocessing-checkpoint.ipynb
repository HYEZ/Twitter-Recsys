{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "1. Topic Extraction\n",
    "2. User Buckets\n",
    "3. Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make user buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import gc\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (FloatType, DateType, StructType, StructField, StringType, LongType,\n",
    "                               IntegerType, ArrayType, BooleanType, DoubleType)\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, QuantileDiscretizer\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "gc.enable()\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.sql.shuffle.partitions\", 500).appName(\"twitter\").getOrCreate()\n",
    "print(spark.sparkContext.getConf().get('spark.driver.memory'))\n",
    "print(spark.sparkContext.getConf().get(\"spark.sql.shuffle.partitions\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validator(df):\n",
    "    columns_w_nan = {}\n",
    "    for col in df.schema:\n",
    "        null_count = df.filter(F.col(col.name).isNull()).count()\n",
    "        if null_count>0:\n",
    "            columns_w_nan[col.name]=null_count\n",
    "    return columns_w_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_size={\"final-complete\": {\"val_size\": 500000, \n",
    "                                    \"train_size\": \"all\"}}\n",
    "\n",
    "training = True\n",
    "submission = False\n",
    "test = False\n",
    "\n",
    "bucket='bucket-name'\n",
    "s3_resource = boto3.resource('s3')\n",
    "top_k_languages = 30\n",
    "top_k_domains = 3000\n",
    "top_k_hashtags = 13000\n",
    "\n",
    "# Embeddings\n",
    "num_partitions=1000\n",
    "\n",
    "# Buckets\n",
    "partition_per_cluster = 100\n",
    "\n",
    "suffix_sample = \"final-complete\" #\"full\", \"small\", \"medium\", \"sub_medium\"\n",
    "data_path = \"final-data\"\n",
    "object_paths = \"final-artifacts\"\n",
    "\n",
    "val_size = dictionary_size[suffix_sample][\"val_size\"]\n",
    "train_size = dictionary_size[suffix_sample][\"train_size\"]\n",
    "\n",
    "bucket_s3 = s3_resource.Bucket(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S3\n",
    "twitter_bucket_s3 = \"s3a://bucket-name\"\n",
    "twitter_bucket_s3 = \"/Volumes/Seagate\\ Backup\\ Plus\\ Drive/ACM_RecSys/\"\n",
    "\n",
    "# trainining_path = os.path.join(twitter_bucket_s3, \"data\", \"raw\", \"final\", \"training.tsv\")\n",
    "# submission_path = os.path.join(twitter_bucket_s3, \"data\", \"raw\", \"final\", \"submissivalon.tsv\")\n",
    "# test_path = os.path.join(twitter_bucket_s3, \"data\", \"raw\", \"final\", \"test.tsv\")\n",
    "trainining_path = \"/Volumes/Seagate\\ Backup\\ Plus\\ Drive/ACM_RecSys/val.tsv\"\n",
    "submission_path = \"/Volumes/Seagate\\ Backup\\ Plus\\ Drive/ACM_RecSys/val.tsv\"\n",
    "test_path = \"/Volumes/Seagate\\ Backup\\ Plus\\ Drive/ACM_RecSys/test.tsv\"\n",
    "\n",
    "# Splitted paths\n",
    "train_path = os.path.join(twitter_bucket_s3, data_path, \"train-\"+suffix_sample)\n",
    "val_path = os.path.join(twitter_bucket_s3, data_path, \"val-\"+suffix_sample)\n",
    "\n",
    "# Processed\n",
    "processed_train_path = os.path.join(twitter_bucket_s3, data_path, \"processed\", \"train-\"+suffix_sample)\n",
    "processed_val_path = os.path.join(twitter_bucket_s3, data_path, \"processed\", \"val-\"+suffix_sample)\n",
    "processed_submission_path = os.path.join(twitter_bucket_s3, data_path, \"processed\", \"submission-\"+suffix_sample)\n",
    "processed_test_path = os.path.join(twitter_bucket_s3, data_path, \"processed\", \"test-\"+suffix_sample)\n",
    "processed_emb_train_path = os.path.join(twitter_bucket_s3, data_path, \"processed-embeddings-final\", \n",
    "                                        \"train-\"+suffix_sample)\n",
    "processed_emb_val_path = os.path.join(twitter_bucket_s3, data_path, \"processed-embeddings-final\", \n",
    "                                      \"val-\"+suffix_sample)\n",
    "processed_emb_submission_path = os.path.join(twitter_bucket_s3, data_path, \"processed-embeddings-final\", \n",
    "                                         \"submission-\"+suffix_sample)\n",
    "processed_emb_test_path = os.path.join(twitter_bucket_s3, data_path, \"processed-embeddings-final\", \n",
    "                                         \"test-\"+suffix_sample)\n",
    "processed_top_train_path = os.path.join(twitter_bucket_s3, data_path, \"processed-topics\", \n",
    "                                        \"train-\"+suffix_sample)\n",
    "processed_top_val_path = os.path.join(twitter_bucket_s3, data_path, \"processed-topics\", \n",
    "                                      \"val-\"+suffix_sample)\n",
    "processed_top_submission_path = os.path.join(twitter_bucket_s3, data_path, \"processed-topics\", \n",
    "                                             \"submission-\"+suffix_sample)\n",
    "processed_top_test_path = os.path.join(twitter_bucket_s3, data_path, \"processed-topics\", \n",
    "                                             \"test-\"+suffix_sample)\n",
    "# Resources\n",
    "engaging_users_training_path = os.path.join(twitter_bucket_s3, data_path, \"engaging-users-training\")\n",
    "engaging_users_submission_path = os.path.join(twitter_bucket_s3, data_path, \"engaging-users-submission\")\n",
    "engaging_users_test_path = os.path.join(twitter_bucket_s3, data_path, \"engaging-users-test\")\n",
    "intentions_path = os.path.join(twitter_bucket_s3, data_path, \"intentions-\"+suffix_sample)\n",
    "map_user_bucket_path = os.path.join(twitter_bucket_s3, data_path, \"map_user_bucket\")\n",
    "\n",
    "topic_encodings_path = os.path.join(twitter_bucket_s3, \"data\", \"textEncodings\", \"user_topics\")\n",
    "users_intime_path = os.path.join(twitter_bucket_s3, data_path, \"users_intime-\"+suffix_sample)\n",
    "\n",
    "# keys objects\n",
    "key_hashtag_mapping = os.path.join(object_paths, f'hashtag_mapping_{suffix_sample}.pkl')\n",
    "key_domain_mapping = os.path.join(object_paths, f'domain_mapping_{suffix_sample}.pkl')\n",
    "key_language_mapping = os.path.join(object_paths, f'language_mapping_{suffix_sample}.pkl')\n",
    "key_hashtag_count = os.path.join(object_paths, f'hashtag_count_{suffix_sample}.pkl')\n",
    "key_domain_count = os.path.join(object_paths, f'domain_count_{suffix_sample}.pkl')\n",
    "key_scaling_features = os.path.join(object_paths, f'scaling_dictionary_{suffix_sample}.pkl')\n",
    "key_diff_min = os.path.join(object_paths, f'diff_min_{suffix_sample}.pkl')\n",
    "key_impute_perc = os.path.join(object_paths, f'dict_mean_perc_{suffix_sample}.pkl')\n",
    "\n",
    "# s3+keys\n",
    "columns = [\"engaged_with_user_follower_count\", \"engaged_with_user_following_count\",\n",
    "           \"engaged_with_user_account_creation\", \"engaging_user_follower_count\",\n",
    "           \"engaging_user_following_count\", \"engaging_user_account_creation\"]\n",
    "qds_paths = {}\n",
    "for col in columns:\n",
    "    qds_paths[col] = os.path.join(twitter_bucket_s3, object_paths, f\"qs_{suffix_sample}_\" + col)\n",
    "    \n",
    "# Bucket pipeline\n",
    "users_buckets = os.path.join(twitter_bucket_s3, data_path, \"users_buckets\") #\n",
    "users_buckets_part_2 = os.path.join(twitter_bucket_s3, data_path, \"users_buckets_part_2\") #\n",
    "\n",
    "pipeline_kmeans_path = os.path.join(twitter_bucket_s3, object_paths, \"pipeline_id_encoding\")\n",
    "cluster_map_path = os.path.join(twitter_bucket_s3, data_path, \"cluster_map\")\n",
    "\n",
    "# Embeddings\n",
    "bert_embeddings_train = os.path.join(twitter_bucket_s3, \"data\", \"textEncodings\", \"tweets_extended\")\n",
    "submission_rawTweetEncodings_path = os.path.join(twitter_bucket_s3, \"data\", \"textEncodings\", \"submissionEmbs.txt\")\n",
    "test_rawTweetEncodings_path = None\n",
    "\n",
    "# Topics pipeline\n",
    "reduced_topics_path = os.path.join(twitter_bucket_s3, \"data\", \"textEncodings\", \"reducedTopics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/Seagate\\\\ Backup\\\\ Plus\\\\ Drive/ACM_RecSys/training.tsv'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainining_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(path='training.tsv', has_labels=True, schema='auto'):\n",
    "    \"\"\"\n",
    "    Parses the training data for the Twitter RecSys Challenge.\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder.appName(\"twitter\").getOrCreate()\n",
    "    if schema == 'auto':\n",
    "        schema = build_schema(has_labels)\n",
    "    df = spark.read.csv(path, schema=schema, sep='\\x01', encoding='utf-8',\n",
    "                        ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True)\n",
    "    df = df.withColumn('text_tokens', F.split('text_tokens', '\\t'))\n",
    "    df = df.withColumn('hashtags', F.split('hashtags', '\\t'))\n",
    "    df = df.withColumn('present_media', F.split('present_media', '\\t'))\n",
    "    df = df.withColumn('present_links', F.split('present_links', '\\t'))\n",
    "    df = df.withColumn('present_domains', F.split('present_domains', '\\t'))\n",
    "    return df\n",
    "\n",
    "def build_schema(has_labels=True):\n",
    "    if has_labels:\n",
    "        schema = StructType([StructField('text_tokens', StringType()),\n",
    "                             StructField('hashtags', StringType()),\n",
    "                             StructField('tweet_id', StringType()),\n",
    "                             StructField('present_media', StringType()),\n",
    "                             StructField('present_links', StringType()),\n",
    "                             StructField('present_domains', StringType()),\n",
    "                             StructField('tweet_type', StringType()),\n",
    "                             StructField('language', StringType()),\n",
    "                             StructField('tweet_timestamp', LongType()),\n",
    "                             StructField('engaged_with_user_id', StringType()),\n",
    "                             StructField('engaged_with_user_follower_count', IntegerType()),\n",
    "                             StructField('engaged_with_user_following_count', IntegerType()),\n",
    "                             StructField('engaged_with_user_is_verified', BooleanType()),\n",
    "                             StructField('engaged_with_user_account_creation', LongType()),\n",
    "                             StructField('engaging_user_id', StringType()),\n",
    "                             StructField('engaging_user_follower_count', IntegerType()),\n",
    "                             StructField('engaging_user_following_count', IntegerType()),\n",
    "                             StructField('engaging_user_is_verified', BooleanType()),\n",
    "                             StructField('engaging_user_account_creation', LongType()),\n",
    "                             StructField('engagee_follows_engager', BooleanType()),\n",
    "                             StructField('reply_timestamp', LongType()),\n",
    "                             StructField('retweet_timestamp', LongType()),\n",
    "                             StructField('retweet_with_comment_timestamp', LongType()),\n",
    "                             StructField('like_timestamp', LongType())\n",
    "                            ])\n",
    "    else:\n",
    "         schema = StructType([StructField('text_tokens', StringType()),\n",
    "                             StructField('hashtags', StringType()),\n",
    "                             StructField('tweet_id', StringType()),\n",
    "                             StructField('present_media', StringType()),\n",
    "                             StructField('present_links', StringType()),\n",
    "                             StructField('present_domains', StringType()),\n",
    "                             StructField('tweet_type', StringType()),\n",
    "                             StructField('language', StringType()),\n",
    "                             StructField('tweet_timestamp', LongType()),\n",
    "                             StructField('engaged_with_user_id', StringType()),\n",
    "                             StructField('engaged_with_user_follower_count', IntegerType()),\n",
    "                             StructField('engaged_with_user_following_count', IntegerType()),\n",
    "                             StructField('engaged_with_user_is_verified', BooleanType()),\n",
    "                             StructField('engaged_with_user_account_creation', LongType()),\n",
    "                             StructField('engaging_user_id', StringType()),\n",
    "                             StructField('engaging_user_follower_count', IntegerType()),\n",
    "                             StructField('engaging_user_following_count', IntegerType()),\n",
    "                             StructField('engaging_user_is_verified', BooleanType()),\n",
    "                             StructField('engaging_user_account_creation', LongType()),\n",
    "                             StructField('engagee_follows_engager', BooleanType())\n",
    "                            ])\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = parse_data(trainining_path, has_labels=True).repartition(600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[text_tokens: array<string>, hashtags: array<string>, tweet_id: string, present_media: array<string>, present_links: array<string>, present_domains: array<string>, tweet_type: string, language: string, tweet_timestamp: bigint, engaged_with_user_id: string, engaged_with_user_follower_count: int, engaged_with_user_following_count: int, engaged_with_user_is_verified: boolean, engaged_with_user_account_creation: bigint, engaging_user_id: string, engaging_user_follower_count: int, engaging_user_following_count: int, engaging_user_is_verified: boolean, engaging_user_account_creation: bigint, engagee_follows_engager: boolean, reply_timestamp: bigint, retweet_timestamp: bigint, retweet_with_comment_timestamp: bigint, like_timestamp: bigint]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b60c1e74ebbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \"\"\"\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(list(bucket_s3.objects.filter(Prefix=f\"{data_path}/users_buckets\", Delimiter='./')))==0:\n",
    "    engaging_user_id_id = training_df.select(F.col(\"engaging_user_id\").alias(\"user_id\"))\n",
    "    engaged_with_user_id_id = training_df.select(F.col(\"engaged_with_user_id\").alias(\"user_id\"))\n",
    "    total_users = engaging_user_id_id.union(engaged_with_user_id_id)\n",
    "    col=\"user_id\"\n",
    "    col_dist = total_users.select(col)\\\n",
    "                          .groupBy(col).count()\n",
    "    \n",
    "    bucketizer_users = col_dist.filter(F.col(\"count\")>=71)\n",
    "    w = Window.orderBy(F.col(\"count\").desc())\n",
    "    bucketizer_users = bucketizer_users.withColumn(\"bucket\", F.row_number().over(w))\n",
    "    bucketizer_users = bucketizer_users.drop(\"count\", \"in_submission\")\n",
    "    \n",
    "    col_dist = col_dist.join(bucketizer_users, on=\"user_id\", how=\"left\")\n",
    "    col_dist.write.csv(users_buckets)\n",
    "else:\n",
    "    print(\"Already exists\")\n",
    "\n",
    "col_dist = spark.read.csv(users_buckets, \n",
    "                          schema=StructType([StructField('user_id', StringType()),\n",
    "                                             StructField('count', IntegerType()),\n",
    "                                             StructField('bucket', IntegerType())]))\n",
    "col_dist = col_dist.orderBy(F.col(\"count\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_buckets = col_dist.select(F.max(\"bucket\")).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training:\n",
    "    if len(list(bucket_s3.objects.filter(Prefix=object_paths+\"/pipeline_id_encoding\", Delimiter='./')))==0:\n",
    "        print(\"Creating Kmeans pipeline\")\n",
    "        engaged_with_df = training_df.select(F.col(\"engaged_with_user_id\").alias(\"user_id\"), \n",
    "                                             F.col(\"engaged_with_user_follower_count\").alias(\"follower_count\"), \n",
    "                                             F.col(\"engaged_with_user_following_count\").alias(\"following_count\"),\n",
    "                                             F.col(\"engaged_with_user_is_verified\").alias(\"is_verified\"), \n",
    "                                             F.col(\"engaged_with_user_account_creation\").alias(\"account_creation\"))\n",
    "        engaging_df = training_df.select(F.col(\"engaging_user_id\").alias(\"user_id\"), \n",
    "                                         F.col(\"engaging_user_follower_count\").alias(\"follower_count\"), \n",
    "                                         F.col(\"engaging_user_following_count\").alias(\"following_count\"),\n",
    "                                         F.col(\"engaging_user_is_verified\").alias(\"is_verified\"), \n",
    "                                         F.col(\"engaging_user_account_creation\").alias(\"account_creation\"))\n",
    "        user_df = engaging_df.union(engaged_with_df)\n",
    "\n",
    "        user_ids_missing = col_dist.select(\"user_id\", \"bucket\").filter(F.col(\"bucket\").isNull()).drop(\"bucket\")\n",
    "        user_kmeans = user_ids_missing.join(user_df, on=\"user_id\", how=\"left\")\n",
    "        grouped = user_kmeans.groupBy(\"user_id\").agg(F.max(F.col(\"follower_count\")).alias(\"follower_count\"), \n",
    "                                                     F.max(F.col(\"following_count\")).alias(\"following_count\"),\n",
    "                                                     F.max(F.col(\"is_verified\")).alias(\"is_verified\"),\n",
    "                                                     F.max(F.col(\"account_creation\")).alias(\"account_creation\"))\n",
    "        cols_for_training = [\"follower_count\", \"following_count\", \"is_verified\", \"account_creation\"]\n",
    "        vectorAssembler = VectorAssembler(inputCols=cols_for_training,\n",
    "                                          outputCol=\"features\")\n",
    "        ss = StandardScaler(inputCol=\"features\", \n",
    "                            outputCol=\"features_ss\", \n",
    "                            withStd=True, \n",
    "                            withMean=True)\n",
    "        clustering = KMeans(k=60, \n",
    "                            featuresCol=\"features_ss\", \n",
    "                            predictionCol=\"cluster\")\n",
    "        pipeline_like = Pipeline(stages=[vectorAssembler, ss, clustering])\n",
    "        model_like = pipeline_like.fit(grouped)\n",
    "        model_like.save(pipeline_kmeans_path)\n",
    "    else:\n",
    "        model_like = PipelineModel.load(pipeline_kmeans_path)\n",
    "        print(\"Kmeans already create\")\n",
    "else:\n",
    "    model_like = PipelineModel.load(pipeline_kmeans_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training:\n",
    "    if len(list(bucket_s3.objects.filter(Prefix=f\"{data_path}/cluster_map\", Delimiter='./')))==0:\n",
    "        preds_cluster = model_like.transform(grouped)\n",
    "        cluster_map = preds_cluster.groupBy(\"cluster\").count()\n",
    "        output = cluster_map.filter(F.col(\"count\")<200).select(F.collect_set(\"cluster\")).collect()\n",
    "        good_clusters = cluster_map.filter(~F.col(\"cluster\").isin(output[0][0]))\n",
    "        good_clusters = good_clusters.withColumn(\"id_cluster\", F.row_number().over(Window.orderBy(F.col(\"count\").desc())))\n",
    "        good_clusters = good_clusters.drop(\"count\")\n",
    "        cluster_map = cluster_map.join(good_clusters, on=\"cluster\", how=\"left\")\n",
    "        cluster_map = cluster_map.withColumn(\"id_cluster\", F.when(F.col(\"id_cluster\").isNotNull(), F.col(\"id_cluster\")).otherwise(0))\n",
    "        cluster_map.write.csv(cluster_map_path)\n",
    "    else:\n",
    "        print(\"Cluster map  already created\")\n",
    "\n",
    "cluster_map = spark.read.csv(cluster_map_path, \n",
    "                             schema=StructType([StructField('cluster', IntegerType()),\n",
    "                                                StructField('count', LongType()),\n",
    "                                                StructField('id_cluster', IntegerType())]))\n",
    "cluster_map = cluster_map.drop(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = parse_data(trainining_path, has_labels=True).repartition(600)\n",
    "submission_df = parse_data(submission_path, has_labels=False).repartition(300)\n",
    "test_df = parse_data(test_path, has_labels=False).repartition(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(list(bucket_s3.objects.filter(Prefix=f\"{data_path}/users_buckets_part_2\", Delimiter='./')))==0:\n",
    "    engaged_with_df = training_df.select(F.col(\"engaged_with_user_id\").alias(\"user_id\"), \n",
    "                                         F.col(\"engaged_with_user_follower_count\").alias(\"follower_count\"), \n",
    "                                         F.col(\"engaged_with_user_following_count\").alias(\"following_count\"),\n",
    "                                         F.col(\"engaged_with_user_is_verified\").alias(\"is_verified\"), \n",
    "                                         F.col(\"engaged_with_user_account_creation\").alias(\"account_creation\"))\n",
    "    engaging_df = training_df.select(F.col(\"engaging_user_id\").alias(\"user_id\"), \n",
    "                                     F.col(\"engaging_user_follower_count\").alias(\"follower_count\"), \n",
    "                                     F.col(\"engaging_user_following_count\").alias(\"following_count\"),\n",
    "                                     F.col(\"engaging_user_is_verified\").alias(\"is_verified\"), \n",
    "                                     F.col(\"engaging_user_account_creation\").alias(\"account_creation\"))\n",
    "    user_df = engaging_df.union(engaged_with_df)\n",
    "\n",
    "    engaged_with_df_sub = submission_df.select(F.col(\"engaged_with_user_id\").alias(\"user_id\"), \n",
    "                                         F.col(\"engaged_with_user_follower_count\").alias(\"follower_count\"), \n",
    "                                         F.col(\"engaged_with_user_following_count\").alias(\"following_count\"),\n",
    "                                         F.col(\"engaged_with_user_is_verified\").alias(\"is_verified\"), \n",
    "                                         F.col(\"engaged_with_user_account_creation\").alias(\"account_creation\"))\n",
    "    engaging_df_sub = submission_df.select(F.col(\"engaging_user_id\").alias(\"user_id\"), \n",
    "                                     F.col(\"engaging_user_follower_count\").alias(\"follower_count\"), \n",
    "                                     F.col(\"engaging_user_following_count\").alias(\"following_count\"),\n",
    "                                     F.col(\"engaging_user_is_verified\").alias(\"is_verified\"), \n",
    "                                     F.col(\"engaging_user_account_creation\").alias(\"account_creation\"))\n",
    "    user_df_sub = engaging_df_sub.union(engaged_with_df_sub)\n",
    "    user_df = user_df.union(user_df_sub) # Union\n",
    "    \n",
    "    engaged_with_df_test = test_df.select(F.col(\"engaged_with_user_id\").alias(\"user_id\"), \n",
    "                                         F.col(\"engaged_with_user_follower_count\").alias(\"follower_count\"), \n",
    "                                         F.col(\"engaged_with_user_following_count\").alias(\"following_count\"),\n",
    "                                         F.col(\"engaged_with_user_is_verified\").alias(\"is_verified\"), \n",
    "                                         F.col(\"engaged_with_user_account_creation\").alias(\"account_creation\"))\n",
    "    engaging_df_test = test_df.select(F.col(\"engaging_user_id\").alias(\"user_id\"), \n",
    "                                     F.col(\"engaging_user_follower_count\").alias(\"follower_count\"), \n",
    "                                     F.col(\"engaging_user_following_count\").alias(\"following_count\"),\n",
    "                                     F.col(\"engaging_user_is_verified\").alias(\"is_verified\"), \n",
    "                                     F.col(\"engaging_user_account_creation\").alias(\"account_creation\"))\n",
    "    user_df_test = engaging_df_test.union(engaged_with_df_test)\n",
    "    user_df = user_df.union(user_df_test) # Union\n",
    "\n",
    "    user_ids_already_bucketized = col_dist.select(F.col(\"user_id\").alias(\"user_bucketized\"), \n",
    "                                                  F.col(\"bucket\")).filter(F.col(\"bucket\").isNotNull()).drop(\"bucket\")\n",
    "    user_kmeans = user_df.join(user_ids_already_bucketized, \n",
    "                               (user_df.user_id==user_ids_already_bucketized.user_bucketized), \n",
    "                               how=\"left\")\n",
    "    user_kmeans = user_kmeans.filter(F.col(\"user_bucketized\").isNull())\n",
    "    grouped = user_kmeans.groupBy(\"user_id\").agg(F.max(F.col(\"follower_count\")).alias(\"follower_count\"),\n",
    "                                                 F.max(F.col(\"following_count\")).alias(\"following_count\"),\n",
    "                                                 F.max(F.col(\"is_verified\")).alias(\"is_verified\"),\n",
    "                                                 F.max(F.col(\"account_creation\")).alias(\"account_creation\"))\n",
    "\n",
    "    preds_cluster= model_like.transform(grouped)\n",
    "    preds_cluster = preds_cluster.join(cluster_map, on=\"cluster\", how=\"left\")\n",
    "    preds_cluster = preds_cluster.withColumn(\"bucket_new\", \n",
    "                                             main_buckets+1+(F.col(\"id_cluster\")*partition_per_cluster)+F.abs(F.hash(F.col(\"user_id\"))%partition_per_cluster))\n",
    "    bucket_new_ids = preds_cluster.select(\"user_id\", \"id_cluster\", \"bucket_new\")\n",
    "    bucket_new_ids = bucket_new_ids.withColumn(\"bucket_new\", F.col(\"bucket_new\").cast(IntegerType()))\n",
    "    bucket_new_ids.write.csv(users_buckets_part_2)\n",
    "# Reading\n",
    "bucket_new_ids = spark.read.csv(users_buckets_part_2, \n",
    "                                schema= StructType([StructField('user_id', StringType()),\n",
    "                                                    StructField('id_cluster', IntegerType()),\n",
    "                                                    StructField('bucket_new', IntegerType())]))\n",
    "bucket_new_ids = bucket_new_ids.drop(\"id_cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if len(list(bucket_s3.objects.filter(Prefix=f\"{data_path}/map_user_bucket\", Delimiter='./')))==0:\n",
    "    map_user_bucket = col_dist.join(bucket_new_ids, on=\"user_id\", how=\"outer\")\n",
    "    map_user_bucket = map_user_bucket.withColumn(\"final_bucket\", F.when(F.col(\"bucket\").isNotNull(), \n",
    "                                                                        F.col(\"bucket\")).otherwise(F.col(\"bucket_new\")))\n",
    "    map_user_bucket = map_user_bucket.select(\"user_id\", \"final_bucket\")\n",
    "    map_user_bucket.write.csv(map_user_bucket_path)\n",
    "    print(\"Map User Bucket created\")\n",
    "    \n",
    "map_user_bucket = spark.read.csv(map_user_bucket_path, \n",
    "                                 schema= StructType([StructField('user_id', StringType()),\n",
    "                                                StructField('final_bucket', IntegerType())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
