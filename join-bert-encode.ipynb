{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "1. Topic Extraction\n",
    "2. User Buckets\n",
    "3. Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "COLS = [\"text_tokens\", \"hashtags\", \"tweet_id\", \"present_media\", \n",
    "                      \"present_links\", \"present_domains\", \"tweet_type\", \"language\", \"tweet_timestamp\", \"engaged_with_user_id\",\n",
    "                     \"engaged_with_user_follower_count\", \"engaged_with_user_following_count\", \n",
    "                     \"engaged_with_user_is_verified\", \"engaged_with_user_account_creation\", \"engaging_user_id\",\n",
    "                     \"engaging_user_follower_count\", \"engaging_user_following_count\", \"engaging_user_is_verified\",\n",
    "                     \"engaging_user_account_creation\", \"engagee_follows_engager\", \"reply_timestamp\",\n",
    "                     \"retweet_timestamp\", \"retweet_with_comment_timestamp\", \"like_timestamp\"]\n",
    "df = pd.read_csv('../dataset/twitter/train100K.csv', names=COLS, skipinitialspace=True, skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>present_media</th>\n",
       "      <th>present_links</th>\n",
       "      <th>present_domains</th>\n",
       "      <th>tweet_type</th>\n",
       "      <th>language</th>\n",
       "      <th>tweet_timestamp</th>\n",
       "      <th>engaged_with_user_id</th>\n",
       "      <th>...</th>\n",
       "      <th>engaging_user_id</th>\n",
       "      <th>engaging_user_follower_count</th>\n",
       "      <th>engaging_user_following_count</th>\n",
       "      <th>engaging_user_is_verified</th>\n",
       "      <th>engaging_user_account_creation</th>\n",
       "      <th>engagee_follows_engager</th>\n",
       "      <th>reply_timestamp</th>\n",
       "      <th>retweet_timestamp</th>\n",
       "      <th>retweet_with_comment_timestamp</th>\n",
       "      <th>like_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101\\t10117\\t140\\t119\\t142\\t119\\t152\\t119\\t1010...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>373C0F43762B7CEC1D75728BE8A33891</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A2CE3A1941BA410A1C31496C355EFCD7</td>\n",
       "      <td>E14AF8A8D257BB47587843FE7D08382B</td>\n",
       "      <td>TopLevel</td>\n",
       "      <td>D3164C7FBCF2565DDF915B1B3AEFB1DC</td>\n",
       "      <td>1582126349</td>\n",
       "      <td>2A8B6AD2B9D55F535C2441AB673133D2</td>\n",
       "      <td>...</td>\n",
       "      <td>00000865A1538142CDA5936B07FE4311</td>\n",
       "      <td>65</td>\n",
       "      <td>166</td>\n",
       "      <td>False</td>\n",
       "      <td>1452599043</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101\\t10105\\t10817\\t10124\\t59232\\t18121\\t15629\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>773A92D9E4824D06105C02BD044BB20A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Quote</td>\n",
       "      <td>D3164C7FBCF2565DDF915B1B3AEFB1DC</td>\n",
       "      <td>1581971193</td>\n",
       "      <td>950A95B81407F33C412E520BE55A1450</td>\n",
       "      <td>...</td>\n",
       "      <td>000009A057792FF118B9E3F2578B8407</td>\n",
       "      <td>1814</td>\n",
       "      <td>1314</td>\n",
       "      <td>False</td>\n",
       "      <td>1322868747</td>\n",
       "      <td>True</td>\n",
       "      <td>1.581979e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101\\t48561\\t10116\\t67737\\t18554\\t36371\\t10989\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>218A6C27871801759F7380D7C41694A6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5C683B5A29B308CADD0D7EFA7C9C32D3</td>\n",
       "      <td>6717B03E03DEE1D7ACAE37649ACA7BD6</td>\n",
       "      <td>TopLevel</td>\n",
       "      <td>9BF3403E0EB7EA8A256DA9019C0B0716</td>\n",
       "      <td>1582047119</td>\n",
       "      <td>ABB2F7F22C34057BC7B30D627B0C137A</td>\n",
       "      <td>...</td>\n",
       "      <td>00000DEF82BE9EB5CFD07FB7DB94317B</td>\n",
       "      <td>4</td>\n",
       "      <td>73</td>\n",
       "      <td>False</td>\n",
       "      <td>1573996260</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101\\t100055\\t69940\\t10414\\t159\\t11305\\t11166\\t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AB817EBA68064A0C8CBF4A6C059D92DC</td>\n",
       "      <td>Photo</td>\n",
       "      <td>E925556EE312213AD98C4D9F131D7A8D</td>\n",
       "      <td>D722330FEBEAAE68B4F4339CE8BD7C70</td>\n",
       "      <td>TopLevel</td>\n",
       "      <td>691890251F2B9FF922BE6D3699ABEFD2</td>\n",
       "      <td>1581554925</td>\n",
       "      <td>03F96C3B7CE2179B6347AA395880C963</td>\n",
       "      <td>...</td>\n",
       "      <td>0000109A57AFA64758EE4AAE2A01BFC7</td>\n",
       "      <td>15</td>\n",
       "      <td>124</td>\n",
       "      <td>False</td>\n",
       "      <td>1385502405</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101\\t62154\\t32221\\t71843\\t10143\\t10237\\t15507\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>349120C1E2801857530393F16D4653A5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TopLevel</td>\n",
       "      <td>9BF3403E0EB7EA8A256DA9019C0B0716</td>\n",
       "      <td>1581568955</td>\n",
       "      <td>E035DCB47CB3DF98C5CD7CFEEC3BC704</td>\n",
       "      <td>...</td>\n",
       "      <td>000012366528B5FEE179A9606DBC9826</td>\n",
       "      <td>1226</td>\n",
       "      <td>655</td>\n",
       "      <td>False</td>\n",
       "      <td>1268639592</td>\n",
       "      <td>True</td>\n",
       "      <td>1.581570e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         text_tokens hashtags  \\\n",
       "0  101\\t10117\\t140\\t119\\t142\\t119\\t152\\t119\\t1010...      NaN   \n",
       "1  101\\t10105\\t10817\\t10124\\t59232\\t18121\\t15629\\...      NaN   \n",
       "2  101\\t48561\\t10116\\t67737\\t18554\\t36371\\t10989\\...      NaN   \n",
       "3  101\\t100055\\t69940\\t10414\\t159\\t11305\\t11166\\t...      NaN   \n",
       "4  101\\t62154\\t32221\\t71843\\t10143\\t10237\\t15507\\...      NaN   \n",
       "\n",
       "                           tweet_id present_media  \\\n",
       "0  373C0F43762B7CEC1D75728BE8A33891           NaN   \n",
       "1  773A92D9E4824D06105C02BD044BB20A           NaN   \n",
       "2  218A6C27871801759F7380D7C41694A6           NaN   \n",
       "3  AB817EBA68064A0C8CBF4A6C059D92DC         Photo   \n",
       "4  349120C1E2801857530393F16D4653A5           NaN   \n",
       "\n",
       "                      present_links                   present_domains  \\\n",
       "0  A2CE3A1941BA410A1C31496C355EFCD7  E14AF8A8D257BB47587843FE7D08382B   \n",
       "1                               NaN                               NaN   \n",
       "2  5C683B5A29B308CADD0D7EFA7C9C32D3  6717B03E03DEE1D7ACAE37649ACA7BD6   \n",
       "3  E925556EE312213AD98C4D9F131D7A8D  D722330FEBEAAE68B4F4339CE8BD7C70   \n",
       "4                               NaN                               NaN   \n",
       "\n",
       "  tweet_type                          language  tweet_timestamp  \\\n",
       "0   TopLevel  D3164C7FBCF2565DDF915B1B3AEFB1DC       1582126349   \n",
       "1      Quote  D3164C7FBCF2565DDF915B1B3AEFB1DC       1581971193   \n",
       "2   TopLevel  9BF3403E0EB7EA8A256DA9019C0B0716       1582047119   \n",
       "3   TopLevel  691890251F2B9FF922BE6D3699ABEFD2       1581554925   \n",
       "4   TopLevel  9BF3403E0EB7EA8A256DA9019C0B0716       1581568955   \n",
       "\n",
       "               engaged_with_user_id  ...                  engaging_user_id  \\\n",
       "0  2A8B6AD2B9D55F535C2441AB673133D2  ...  00000865A1538142CDA5936B07FE4311   \n",
       "1  950A95B81407F33C412E520BE55A1450  ...  000009A057792FF118B9E3F2578B8407   \n",
       "2  ABB2F7F22C34057BC7B30D627B0C137A  ...  00000DEF82BE9EB5CFD07FB7DB94317B   \n",
       "3  03F96C3B7CE2179B6347AA395880C963  ...  0000109A57AFA64758EE4AAE2A01BFC7   \n",
       "4  E035DCB47CB3DF98C5CD7CFEEC3BC704  ...  000012366528B5FEE179A9606DBC9826   \n",
       "\n",
       "   engaging_user_follower_count  engaging_user_following_count  \\\n",
       "0                            65                            166   \n",
       "1                          1814                           1314   \n",
       "2                             4                             73   \n",
       "3                            15                            124   \n",
       "4                          1226                            655   \n",
       "\n",
       "   engaging_user_is_verified engaging_user_account_creation  \\\n",
       "0                      False                     1452599043   \n",
       "1                      False                     1322868747   \n",
       "2                      False                     1573996260   \n",
       "3                      False                     1385502405   \n",
       "4                      False                     1268639592   \n",
       "\n",
       "   engagee_follows_engager  reply_timestamp  retweet_timestamp  \\\n",
       "0                    False              NaN                NaN   \n",
       "1                     True     1.581979e+09                NaN   \n",
       "2                    False              NaN                NaN   \n",
       "3                     True              NaN                NaN   \n",
       "4                     True     1.581570e+09                NaN   \n",
       "\n",
       "   retweet_with_comment_timestamp  like_timestamp  \n",
       "0                             NaN             NaN  \n",
       "1                             NaN             NaN  \n",
       "2                             NaN             NaN  \n",
       "3                             NaN             NaN  \n",
       "4                             NaN             NaN  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 버트 인코딩\n",
    "- 학습가능한 인코딩을 얻기 위해서 토근화된 텍스트를 [16] BERT 모델에 input으로 넣음\n",
    "- 하지만 버트 모델의 학습/추론 시간이 커서, __텍스트 정보들을 인코딩하는 새로운 피처__ 를 만들었음\n",
    "- 즉, 버트를 사용해서 텍스트 인코딩 만들고, 학습 불가능한 피처로 사용하는거임\n",
    "- 버트 인코딩을 사용해서 각 트윗으로부터 메인 토픽 추출 -> 한 유저에 대해(유저별로) 가장 많이나타나는 토픽들을 찾아서 결정함\n",
    "- 버트의 파라미터는 업데이트되지만, 이 text token은 한번만 인풋으로 사용함\n",
    "- 즉, 버트 모델을 사용해서 토큰들을 인코딩, 트윗마다 메인 topic 추출함 => 유저마다 가장 많이 보이는 5개의 topic 고르기\n",
    "- 알고리즘1) 버트 모델을 사용해서 토큰들을 인코딩, 트윗마다 메인 topic 추출함\n",
    "    - 버트모델로 토큰들 인코딩\n",
    "    - PCA를 사용해서 분산의 95퍼센트만 유지하여 인코딩의 차원 줄임\n",
    "    - k-means를 사용해서 줄인 인코딩으로부터 150개의 클러스터 생성\n",
    "    - 각 트윗마다 클러스터에 topic 할당함\n",
    "    \n",
    "- https://stackabuse.com/text-classification-with-bert-tokenizer-and-tf-2-0-in-python\n",
    "- https://www.kaggle.com/negedng/bert-embeddings-with-tensorflow-2-0-example\n",
    "- https://huggingface.co/transformers/model_doc/bert.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: bert-for-tf2 in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (0.14.7)\n",
      "Requirement already up-to-date: params-flow in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (0.8.2)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.94-cp36-cp36m-macosx_10_6_x86_64.whl (1.1 MB)\n",
      "Requirement already satisfied, skipping upgrade: py-params>=0.9.6 in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (from bert-for-tf2) (0.9.7)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (from params-flow) (4.48.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (from params-flow) (1.17.2)\n",
      "Installing collected packages: sentencepiece\n",
      "  Attempting uninstall: sentencepiece\n",
      "    Found existing installation: sentencepiece 0.1.91\n",
      "    Uninstalling sentencepiece-0.1.91:\n",
      "      Successfully uninstalled sentencepiece-0.1.91\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "transformers 3.5.1 requires sentencepiece==0.1.91, but you'll have sentencepiece 0.1.94 which is incompatible.\u001b[0m\n",
      "Successfully installed sentencepiece-0.1.94\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade bert-for-tf2 params-flow sentencepiece #>> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-for-tf2 in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (0.14.7)\n",
      "Requirement already satisfied: params-flow>=0.8.0 in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (from bert-for-tf2) (0.8.2)\n",
      "Requirement already satisfied: py-params>=0.9.6 in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (from bert-for-tf2) (0.9.7)\n",
      "Requirement already satisfied: numpy in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.17.2)\n",
      "Requirement already satisfied: tqdm in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.48.2)\n",
      "Requirement already satisfied: sentencepiece in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (0.1.94)\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-for-tf2\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert import tokenization\n",
    "BertTokenizer = tokenization.FullTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "import bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertTokenizer = bert.tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = df['text_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000,)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-hub-nightly\n",
      "  Downloading tf_hub_nightly-0.10.0.dev202011190005-py2.py3-none-any.whl (107 kB)\n",
      "\u001b[K     |████████████████████████████████| 107 kB 668 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (from tf-hub-nightly) (1.17.2)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (from tf-hub-nightly) (3.13.0)\n",
      "Requirement already satisfied: six>=1.9 in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (from protobuf>=3.8.0->tf-hub-nightly) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (from protobuf>=3.8.0->tf-hub-nightly) (49.6.0.post20200814)\n",
      "Installing collected packages: tf-hub-nightly\n",
      "Successfully installed tf-hub-nightly-0.10.0.dev202011190005\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-hub-nightly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version:  2.3.0\n",
      "Hub version:  0.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "print(\"TF version: \", tf.__version__)\n",
    "print(\"Hub version: \", hub.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from bert.tokenization import FullTokenizer     # Still from bert module\n",
    "from tensorflow.keras.models import Model       # Keras is the new high level API for TensorFlow\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 128  # Your choice here.\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=True)\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[pooled_output, sequence_output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See BERT paper: https://arxiv.org/pdf/1810.04805.pdf\n",
    "# And BERT implementation convert_single_example() at https://github.com/google-research/bert/blob/master/run_classifier.py\n",
    "\n",
    "def get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab don't need this. FullTokenizer is not updated to tf2.0 yet\n",
    "tf.gfile = tf.io.gfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # s = \"This is a nice sentence.\"\n",
    "# # stokens = tokenizer.tokenize(s)\n",
    "\n",
    "# input_ids = get_ids(stokens, tokenizer, max_seq_length)\n",
    "# input_masks = get_masks(stokens, max_seq_length)\n",
    "# input_segments = get_segments(stokens, max_seq_length)\n",
    "\n",
    "# input_ids = text_tokens\n",
    "# input_masks = get_masks(te)\n",
    "\n",
    "# # print(stokens)\n",
    "# print(input_ids)\n",
    "# print(input_masks)\n",
    "# print(input_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_rooted(x):\n",
    "    return math.sqrt(sum([a*a for a in x]))\n",
    "\n",
    "\n",
    "def cosine_similarity(x,y):\n",
    "    numerator = sum(a*b for a,b in zip(x,y))\n",
    "    denominator = square_rooted(x)*square_rooted(y)\n",
    "    return numerator/float(denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-82ba7e20d77c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpool_embs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_embs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_masks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_segments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'input_ids' is not defined"
     ]
    }
   ],
   "source": [
    "pool_embs, all_embs = model.predict([[input_ids],[input_masks],[input_segments]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.3.0\n",
      "Hub version:  0.10.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import bert\n",
    "from tensorflow.keras.models import  Model\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "print(\"TensorFlow Version:\",tf.__version__)\n",
    "print(\"Hub version: \",hub.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_layer=hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN=128\n",
    "input_word_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks(tokens, max_seq_length):\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "FullTokenizer=bert.tokenization.FullTokenizer\n",
    "\n",
    "vocab_file=bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "\n",
    "do_lower_case=bert_layer.resolved_object.do_lower_case.numpy()\n",
    "\n",
    "tokenizer=FullTokenizer(vocab_file,do_lower_case)\n",
    "\n",
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens,)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['101\\t10117\\t140\\t119\\t142\\t119\\t152\\t119\\t10108\\t138\\t37615\\t13969\\t117\\t10105\\t100986\\t86153\\t24166\\t17155\\t10105\\t36474\\t34582\\t10251\\t18421\\t118\\t11135\\t97992\\t72894\\t117\\t34208\\t11850\\t169\\t16624\\t10908\\t118\\t32684\\t10162\\t100986\\t10343\\t118\\t11735\\t63803\\t10115\\t117\\t11284\\t10238\\t10114\\t108672\\t10189\\t169\\t18064\\t56162\\t10108\\t10380\\t12097\\t14191\\t32941\\t10107\\t10944\\t10347\\t77298\\t10162\\t10155\\t12864\\t64502\\t119\\t14120\\t131\\t120\\t120\\t188\\t119\\t11170\\t120\\t139\\t11305\\t10410\\t11305\\t10123\\t11403\\t11011\\t11274\\t10107\\t11011\\t102'\n",
      " '101\\t10105\\t10817\\t10124\\t59232\\t18121\\t15629\\t119\\t119\\t102'\n",
      " '101\\t48561\\t10116\\t67737\\t18554\\t36371\\t10989\\t31288\\t117\\t19604\\t13996\\t19455\\t37605\\t70427\\t35794\\t119\\t29005\\t61601\\t17843\\t19455\\t37058\\t17640\\t49770\\t10588\\t10483\\t53522\\t19604\\t10676\\t34743\\t14089\\t12591\\t48973\\t56901\\t10115\\t10120\\t35181\\t35639\\t23607\\t119\\t14120\\t131\\t120\\t120\\t188\\t119\\t11170\\t120\\t124\\t15417\\t15417\\t10729\\t10113\\t12396\\t11447\\t11305\\t10107\\t10858\\t102'\n",
      " ...\n",
      " '101\\t56898\\t137\\t30350\\t83452\\t11447\\t33876\\t10729\\t131\\t14120\\t131\\t120\\t120\\t188\\t119\\t11170\\t120\\t178\\t10161\\t51960\\t10350\\t91995\\t99903\\t11703\\t102'\n",
      " '101\\t107\\t17160\\t11951\\t10399\\t10525\\t13028\\t10106\\t10105\\t13295\\t10108\\t10491\\t10874\\t31572\\t16405\\t18713\\t10108\\t10105\\t12931\\t112\\t187\\t75980\\t51777\\t10111\\t104862\\t10189\\t12277\\t12153\\t10106\\t14301\\t10261\\t11337\\t17734\\t10114\\t48394\\t20796\\t103633\\t10106\\t10105\\t13451\\t13685\\t117\\t10114\\t18926\\t10114\\t10173\\t48175\\t10123\\t10135\\t11408\\t89082\\t10108\\t10796\\t76484\\t10269\\t10426\\t10111\\t10114\\t51600\\t10188\\t14301\\t16437\\t147\\t119\\t29846\\t119\\t119\\t119\\t107\\t14120\\t131\\t120\\t120\\t188\\t119\\t11170\\t120\\t130\\t67403\\t11305\\t10237\\t10410\\t10477\\t11090\\t39900\\t102'\n",
      " '101\\t56898\\t137\\t20179\\t168\\t83086\\t10171\\t131\\t10117\\t100\\t10685\\t10237\\t67912\\t10112\\t117\\t11580\\t10901\\t16542\\t10105\\t12786\\t23010\\t100\\t59099\\t78978\\t14120\\t131\\t120\\t120\\t188\\t119\\t11170\\t120\\t128\\t11447\\t10237\\t11403\\t11281\\t11733\\t10929\\t10116\\t11565\\t11090\\t102']\n"
     ]
    }
   ],
   "source": [
    "train_sentences = df[\"text_tokens\"].fillna(\"CVxTz\").values\n",
    "print(train_sentences)\n",
    "# list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "list_classes = range(150)\n",
    "# train_y = df[list_classes].values\n",
    "train_y = list_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_single_input(sentence,MAX_LEN):\n",
    "  \n",
    "    stokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "    stokens = stokens[:MAX_LEN]\n",
    "\n",
    "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    "\n",
    "    ids = get_ids(stokens, tokenizer, MAX_SEQ_LEN)\n",
    "    masks = get_masks(stokens, MAX_SEQ_LEN)\n",
    "    segments = get_segments(stokens, MAX_SEQ_LEN)\n",
    "\n",
    "    return ids,masks,segments\n",
    "\n",
    "def create_input_array(sentences):\n",
    "\n",
    "    input_ids, input_masks, input_segments = [], [], []\n",
    "\n",
    "    for sentence in tqdm(sentences,position=0, leave=True):\n",
    "\n",
    "        ids,masks,segments=create_single_input(sentence,MAX_SEQ_LEN-2)\n",
    "\n",
    "        input_ids.append(ids)\n",
    "        input_masks.append(masks)\n",
    "        input_segments.append(segments)\n",
    "\n",
    "    return [np.asarray(input_ids, dtype=np.int32), \n",
    "            np.asarray(input_masks, dtype=np.int32), \n",
    "            np.asarray(input_segments, dtype=np.int32)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-3.5.1-py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers==0.9.3\n",
      "  Downloading tokenizers-0.9.3-cp36-cp36m-macosx_10_11_x86_64.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (from transformers) (4.48.2)\n",
      "Collecting sentencepiece==0.1.91\n",
      "  Using cached sentencepiece-0.1.91-cp36-cp36m-macosx_10_6_x86_64.whl (1.1 MB)\n",
      "Requirement already satisfied: filelock in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (from transformers) (2020.7.14)\n",
      "Requirement already satisfied: requests in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: numpy in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (from transformers) (1.17.2)\n",
      "Collecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Processing /Users/ohyeji/Library/Caches/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2/sacremoses-0.0.43-py3-none-any.whl\n",
      "Requirement already satisfied: protobuf in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (from transformers) (3.13.0)\n",
      "Requirement already satisfied: packaging in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (from requests->transformers) (1.25.10)\n",
      "Requirement already satisfied: joblib in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: click in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (from protobuf->transformers) (49.6.0.post20200814)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/ohyeji/anaconda3/lib/python3.6/site-packages (from packaging->transformers) (2.4.7)\n",
      "Installing collected packages: tokenizers, sentencepiece, dataclasses, sacremoses, transformers\n",
      "  Attempting uninstall: sentencepiece\n",
      "    Found existing installation: sentencepiece 0.1.94\n",
      "    Uninstalling sentencepiece-0.1.94:\n",
      "      Successfully uninstalled sentencepiece-0.1.94\n",
      "Successfully installed dataclasses-0.8 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "from transformers import BertModel, BertConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = BertConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nBertModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-5dd61da47a53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfiguration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/transformers/utils/dummy_pt_objects.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m         \u001b[0mrequires_pytorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mrequires_pytorch\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__name__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPYTORCH_IMPORT_ERROR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: \nBertModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\n"
     ]
    }
   ],
   "source": [
    "model = BertModel(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
